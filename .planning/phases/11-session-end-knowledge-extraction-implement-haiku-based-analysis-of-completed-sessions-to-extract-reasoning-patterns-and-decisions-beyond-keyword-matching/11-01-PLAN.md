---
phase: 11-session-end-knowledge-extraction
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/session-analyzer.js
  - get-shit-done/bin/analysis-prompts.js
  - mcp-servers/telegram-mcp/src/storage/question-queue.ts
autonomous: true

must_haves:
  truths:
    - "Session analyzer extracts decisions, reasoning patterns, and meta-knowledge from session JSONL entries via Haiku Task() subagent (zero direct API calls)"
    - "Prompt templates produce structured JSON output with context grounding (context_snippet required)"
    - "conversation_id field tracks related messages in question-queue entries and session JSONL"
  artifacts:
    - path: "get-shit-done/bin/session-analyzer.js"
      provides: "Core session analysis orchestrator with Haiku Task() integration"
      contains: "analyzeSession"
    - path: "get-shit-done/bin/analysis-prompts.js"
      provides: "Three extraction prompt templates (decision, reasoning, meta-knowledge) with JSON output schema"
      contains: "DECISION_EXTRACTION_PROMPT"
    - path: "mcp-servers/telegram-mcp/src/storage/question-queue.ts"
      provides: "conversation_id field in PendingQuestion interface"
      contains: "conversation_id"
  key_links:
    - from: "get-shit-done/bin/session-analyzer.js"
      to: "get-shit-done/bin/analysis-prompts.js"
      via: "require for prompt templates"
      pattern: "require.*analysis-prompts"
    - from: "get-shit-done/bin/session-analyzer.js"
      to: "mcp-servers/telegram-mcp/src/storage/session-manager"
      via: "loadSessionJSONL for reading session entries"
      pattern: "loadSessionJSONL"
---

<objective>
Create the core session analysis infrastructure: a session analyzer module that uses Haiku via Claude Code Task() subagent to extract decisions, reasoning patterns, and meta-knowledge from session JSONL files, plus conversation_id grouping for multi-message conversations.

Purpose: Establishes the extraction engine that all other Phase 11 plans depend on. The analyzer reads session JSONL entries, formats them for Haiku analysis, and returns structured insights. Conversation grouping enables multi-message tracking per user decision.

Output: session-analyzer.js (orchestrator), analysis-prompts.js (3 prompt templates), updated question-queue.ts (conversation_id field)
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-CONTEXT.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-RESEARCH.md
@mcp-servers/telegram-mcp/src/storage/session-manager.ts
@mcp-servers/telegram-mcp/src/storage/question-queue.ts
@get-shit-done/bin/knowledge-extraction.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create session analyzer and prompt templates</name>
  <files>
    get-shit-done/bin/session-analyzer.js
    get-shit-done/bin/analysis-prompts.js
  </files>
  <action>
Create two new modules:

**analysis-prompts.js** - Three Haiku extraction prompt templates:

1. `DECISION_EXTRACTION_PROMPT` - Extracts explicit decisions from session transcript. Output schema: `[{type:"decision", decision:string, reasoning:string, alternatives_considered:string[], confidence:"high"|"medium"|"low", context_snippet:string}]`. Include instruction: "Only extract decisions EXPLICITLY stated in transcript. If uncertain, set confidence to low." Use temperature 0.3 guidance in prompt metadata.

2. `REASONING_PATTERN_PROMPT` - Extracts reasoning patterns (chain-of-thought, debugging, tradeoff-analysis). Output schema: `[{type:"reasoning_pattern", pattern_type:string, description:string, trigger:string, outcome:string, reusable:boolean, context_snippet:string}]`. Include instruction to look for multi-step reasoning chains, problem-solving approaches, and meta-reasoning.

3. `META_KNOWLEDGE_PROMPT` - Extracts higher-order insights (preferences, principles, constraints, learning patterns). Output schema: `[{type:"meta_knowledge", category:"preference"|"principle"|"constraint"|"learning_pattern", statement:string, evidence:string[], confidence:"high"|"medium"|"low", scope:"project"|"global", context_snippet:string}]`.

Each prompt template is a function that accepts `sessionText` (formatted entries string) and returns the complete prompt string with `{{SESSION_ENTRIES}}` replaced. Export `buildDecisionPrompt(sessionText)`, `buildReasoningPrompt(sessionText)`, `buildMetaKnowledgePrompt(sessionText)`.

Also export a `formatEntriesForPrompt(entries)` function that:
- Filters to only question, answer, user_message, bot_response entry types (skips session_metadata, heartbeat, session_close)
- Formats each entry as `[{index}] [{timestamp}] {TYPE}: {content}` with context on new line if present
- Returns joined string with double newlines between entries

**session-analyzer.js** - Core analyzer orchestrator:

`analyzeSession(sessionEntries, options)` function where options = `{extractDecisions:true, extractReasoning:true, extractMetaKnowledge:true}`. The function:

1. Calls `formatEntriesForPrompt(sessionEntries)` to prepare text
2. For each enabled extraction type, builds prompt via analysis-prompts.js
3. **CRITICAL: Uses Claude Code Task() subagent mechanism, NOT direct API calls.** The analyzer is designed to be invoked by a GSD tool/workflow that spawns Haiku subagents. Implementation pattern: export the prompts and formatted text so the calling workflow can pass them to `Task(subagent_type="general-purpose", model="haiku", prompt=...)`. The analyzer itself prepares the extraction requests as an array of `{type, prompt, parseSchema}` objects.
4. Returns array of `{type, prompt, expectedSchema}` extraction requests (the caller handles Task() invocation)

Also export `parseExtractionResult(type, rawJsonText)` that:
- Attempts JSON.parse on raw text (handles markdown code blocks wrapping)
- Validates array structure
- Filters entries missing required fields (decision text, context_snippet, etc.)
- Returns `{insights: validInsights[], errors: string[]}`

Use CommonJS (`module.exports`) to match existing get-shit-done/bin/ patterns.
  </action>
  <verify>
    node -e "const a = require('/Users/ollorin/get-shit-done/get-shit-done/bin/analysis-prompts.js'); console.log(Object.keys(a)); const s = require('/Users/ollorin/get-shit-done/get-shit-done/bin/session-analyzer.js'); console.log(Object.keys(s));"
  </verify>
  <done>
    session-analyzer.js exports analyzeSession and parseExtractionResult. analysis-prompts.js exports buildDecisionPrompt, buildReasoningPrompt, buildMetaKnowledgePrompt, and formatEntriesForPrompt. All functions use subscription-only Task() pattern (no @anthropic-ai/sdk imports). Prompts require context_snippet for grounding.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add conversation_id to question queue and session entries</name>
  <files>
    mcp-servers/telegram-mcp/src/storage/question-queue.ts
  </files>
  <action>
Update PendingQuestion interface in question-queue.ts to add conversation_id support per locked user decision #2:

1. Add `conversation_id?: string` field to `PendingQuestion` interface (optional for backward compatibility)

2. Update `appendQuestion` function signature to accept optional `conversation_id` in the question parameter: `question: { question: string; context?: string; conversation_id?: string }`. Pass it through to the fullQuestion object.

3. Add new exported function `loadConversationMessages(sessionId: string, conversationId: string): Promise<SessionEntry[]>` that:
   - Loads session JSONL via loadSessionJSONL
   - Filters entries to those with matching conversation_id
   - Sorts by timestamp chronologically
   - Returns the filtered, sorted entries

4. Add new exported function `getConversationEntries(sessionId: string): Map<string, SessionEntry[]>` that:
   - Loads all session entries
   - Groups entries by conversation_id (entries without conversation_id grouped under "ungrouped")
   - Returns Map of conversationId -> entries[]

After TypeScript changes, rebuild: `cd mcp-servers/telegram-mcp && npm run build`
  </action>
  <verify>
    cd /Users/ollorin/get-shit-done/mcp-servers/telegram-mcp && npm run build 2>&1 | tail -5
  </verify>
  <done>
    PendingQuestion interface includes conversation_id field. appendQuestion accepts and stores conversation_id. loadConversationMessages and getConversationEntries functions exported for grouping multi-message conversations. TypeScript compiles without errors.
  </done>
</task>

</tasks>

<verification>
1. `node -e "require('./get-shit-done/bin/session-analyzer.js')"` loads without error
2. `node -e "require('./get-shit-done/bin/analysis-prompts.js')"` loads without error
3. `node -e "const p = require('./get-shit-done/bin/analysis-prompts.js'); console.log(p.buildDecisionPrompt('test').includes('SESSION_ENTRIES') === false)"` returns true (template substitution works)
4. `cd mcp-servers/telegram-mcp && npm run build` compiles without errors
5. No imports of `@anthropic-ai/sdk` in any new files
</verification>

<success_criteria>
- Session analyzer module prepares Haiku extraction requests without direct API calls
- Three specialized prompt templates produce structured JSON with context grounding
- conversation_id field tracks multi-message conversations in session storage
- All code follows existing codebase patterns (CommonJS for bin/, TypeScript for MCP)
</success_criteria>

<output>
After completion, create `.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-01-SUMMARY.md`
</output>
