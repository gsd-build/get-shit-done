---
phase: 11-session-end-knowledge-extraction
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/session-quality-gate.js
autonomous: true

must_haves:
  truths:
    - "Small or trivial sessions (fewer than 2 Q&A exchanges or fewer than 10 entries) are skipped to avoid wasting Haiku API costs"
    - "Sessions exceeding 25k characters are chunked into segments for independent analysis"
    - "Already-analyzed sessions are detected via content hash and not re-analyzed"
  artifacts:
    - path: "get-shit-done/bin/session-quality-gate.js"
      provides: "Session filtering, chunking, and re-analysis prevention"
      exports: ["shouldAnalyzeSession", "chunkSessionEntries", "computeSessionHash", "isAlreadyAnalyzed", "markAnalyzed"]
  key_links:
    - from: "get-shit-done/bin/session-quality-gate.js"
      to: "session JSONL files"
      via: "reads entries to evaluate quality"
      pattern: "entries\\.filter"
    - from: "get-shit-done/bin/session-quality-gate.js"
      to: "analysis tracking file"
      via: "reads/writes .planning/analysis-tracking.json"
      pattern: "analysis-tracking\\.json"
---

<objective>
Build session quality gates and chunking logic to control when and how sessions are analyzed, preventing wasted API costs on trivial sessions and handling large sessions gracefully.

Purpose: Not every session deserves Haiku analysis. Test sessions, quick one-off questions, and already-analyzed sessions should be skipped. Large sessions need chunking to stay within Haiku context limits.

Output: `session-quality-gate.js` module with filtering, chunking, and tracking functions.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create session quality gate and chunking module</name>
  <files>get-shit-done/bin/session-quality-gate.js</files>
  <action>
Create `session-quality-gate.js` as a CommonJS module with quality filtering, chunking, and tracking.

**Quality Gate** `shouldAnalyzeSession(entries, options = {})`:
- Count entry types: questions, answers, user_messages, bot_responses
- Filter criteria (all must pass):
  - At least 2 question entries AND at least 2 answer entries (meaningful Q&A interaction)
  - At least 10 total entries (not counting heartbeats)
  - Total content length (all question/answer/message text combined) >= 200 characters
- Options allow overriding thresholds: `{ minQuestions: 2, minAnswers: 2, minEntries: 10, minContentLength: 200 }`
- Return `{ shouldAnalyze: boolean, reason: string, stats: { questions, answers, entries, contentLength } }`

**Chunking** `chunkSessionEntries(entries, options = {})`:
- Use `formatEntriesForPrompt` logic inline (don't import session-analyzer to avoid circular deps) â€” filter to content entries, format as text
- Default max chunk size: 25000 characters (from Claude Code /insights research)
- Options: `{ maxChunkSize: 25000, overlapEntries: 2 }`
- Chunking strategy:
  - Format all relevant entries to text
  - If total text <= maxChunkSize, return single chunk `[{ entries, text, chunkIndex: 0, totalChunks: 1 }]`
  - Otherwise, accumulate entries until text exceeds maxChunkSize, then split
  - Include `overlapEntries` entries from end of previous chunk at start of next (context preservation)
  - Each chunk: `{ entries: SessionEntry[], text: string, chunkIndex: number, totalChunks: number }`
- Return array of chunks

**Session Hash** `computeSessionHash(entries)`:
- Compute SHA-256 of concatenated content entries (questions + answers + messages, sorted by timestamp)
- Skip heartbeats and metadata in hash computation
- Return hex string

**Analysis Tracking** `isAlreadyAnalyzed(sessionHash)` and `markAnalyzed(sessionHash, sessionId, insightCount)`:
- Track analyzed sessions in `.planning/analysis-tracking.json`
- File structure: `{ analyzed: { [hash]: { session_id, analyzed_at, insight_count } } }`
- `isAlreadyAnalyzed` reads file, checks if hash exists
- `markAnalyzed` reads file, adds hash entry, writes back
- If file doesn't exist, create with empty `{ analyzed: {} }`
- Use `require('fs')` sync operations (tracking file is small, no lock needed)
- Limit tracking file to 500 entries max (FIFO eviction by analyzed_at when exceeded)

All functions are synchronous except where noted. No external dependencies.
  </action>
  <verify>
Run:
```bash
node -e "
const g = require('./get-shit-done/bin/session-quality-gate.js');
console.log(Object.keys(g));

// Test quality gate - too few entries
const r1 = g.shouldAnalyzeSession([{type:'question',question:'hi'},{type:'answer',answer:'hello'}]);
console.log('skip trivial:', !r1.shouldAnalyze);

// Test quality gate - enough entries
const entries = [];
for (let i = 0; i < 12; i++) {
  entries.push({type:'question',question:'What about topic ' + i + ' in detail?',timestamp:'2026-01-01T00:0' + i + ':00Z'});
  entries.push({type:'answer',answer:'Here is a detailed answer about topic ' + i + ' with technical content.',timestamp:'2026-01-01T00:0' + i + ':30Z'});
}
const r2 = g.shouldAnalyzeSession(entries);
console.log('analyze good:', r2.shouldAnalyze);

// Test chunking - single chunk
const c1 = g.chunkSessionEntries(entries);
console.log('single chunk:', c1.length === 1);

// Test hash
const h = g.computeSessionHash(entries);
console.log('hash length:', h.length === 64);

// Test tracking
console.log('not analyzed:', !g.isAlreadyAnalyzed('test_hash_123'));
"
```
Should show exported keys, skip trivial=true, analyze good=true, single chunk=true, hash length=true, not analyzed=true.
  </verify>
  <done>Quality gate correctly filters trivial sessions, chunking splits large sessions at 25k-char boundaries with overlap, content hash enables re-analysis prevention, tracking file persists analysis history.</done>
</task>

</tasks>

<verification>
1. Module loads without errors: `node -e "require('./get-shit-done/bin/session-quality-gate.js'); console.log('OK')"`
2. Quality gate rejects sessions with fewer than 2 Q&A exchanges
3. Quality gate accepts sessions with sufficient content
4. Chunking returns single chunk for small sessions
5. Session hash is deterministic (same entries = same hash)
6. Analysis tracking persists to disk
</verification>

<success_criteria>
Quality gate module prevents wasted Haiku API calls on trivial sessions, chunks large sessions for multi-part analysis, and tracks already-analyzed sessions to avoid redundant costs.
</success_criteria>

<output>
After completion, create `.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-02-SUMMARY.md`
</output>
