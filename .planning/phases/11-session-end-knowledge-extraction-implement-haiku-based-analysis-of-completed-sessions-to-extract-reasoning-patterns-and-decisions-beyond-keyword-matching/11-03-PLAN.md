---
phase: 11-session-end-knowledge-extraction
plan: 03
type: execute
wave: 2
depends_on: ["11-01", "11-02"]
files_modified:
  - get-shit-done/bin/session-knowledge-writer.js
  - mcp-servers/telegram-mcp/src/index.ts
autonomous: true

must_haves:
  truths:
    - "Haiku-extracted insights are stored in the Phase 3 knowledge system with proper types, TTL categories, and deduplication"
    - "Session close in MCP server triggers async analysis that does not block shutdown"
    - "Analysis failures are logged but never prevent session close or MCP server shutdown"
  artifacts:
    - path: "get-shit-done/bin/session-knowledge-writer.js"
      provides: "Bridge between session analyzer output and knowledge system storage"
      exports: ["storeSessionInsights", "analyzeAndStoreSession"]
    - path: "mcp-servers/telegram-mcp/src/index.ts"
      provides: "Session-end hook triggering Haiku analysis"
  key_links:
    - from: "get-shit-done/bin/session-knowledge-writer.js"
      to: "get-shit-done/bin/knowledge.js"
      via: "knowledge.add() and knowledge.search() for dedup"
      pattern: "knowledge\\.add|knowledge\\.safeAdd"
    - from: "get-shit-done/bin/session-knowledge-writer.js"
      to: "get-shit-done/bin/session-analyzer.js"
      via: "analyzeSession import"
      pattern: "require.*session-analyzer"
    - from: "get-shit-done/bin/session-knowledge-writer.js"
      to: "get-shit-done/bin/session-quality-gate.js"
      via: "shouldAnalyzeSession, isAlreadyAnalyzed"
      pattern: "require.*session-quality-gate"
    - from: "mcp-servers/telegram-mcp/src/index.ts"
      to: "get-shit-done/bin/session-knowledge-writer.js"
      via: "child_process.fork or spawn for async analysis"
      pattern: "child_process|spawn|fork"
---

<objective>
Wire the session analyzer to the knowledge system and trigger it from the MCP server's session close lifecycle, creating the end-to-end pipeline: session close -> quality gate -> Haiku analysis -> knowledge storage.

Purpose: Complete the integration loop so knowledge extraction happens automatically when sessions end, with proper dedup against existing knowledge and non-blocking execution.

Output: `session-knowledge-writer.js` (storage bridge) and updated `index.ts` (session-end hook).
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-RESEARCH.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-01-SUMMARY.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-02-SUMMARY.md
@get-shit-done/bin/knowledge.js
@get-shit-done/bin/knowledge-dedup.js
@get-shit-done/bin/knowledge-evolution.js
@get-shit-done/bin/embeddings.js
@mcp-servers/telegram-mcp/src/index.ts
@mcp-servers/telegram-mcp/src/storage/session-manager.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create session knowledge writer module</name>
  <files>get-shit-done/bin/session-knowledge-writer.js</files>
  <action>
Create `session-knowledge-writer.js` as a CommonJS module that bridges the session analyzer and knowledge system.

**Insight-to-knowledge mapping:**
- `decision` insight -> knowledge type `decision`, TTL category `long_term` (90 days), scope `project`
- `reasoning_pattern` insight -> knowledge type `lesson`, TTL category `permanent`, scope `project`
- `meta_knowledge` insight -> knowledge type based on category:
  - `preference` -> type `lesson`, scope `global` (preferences cross projects)
  - `principle` -> type `lesson`, scope `global`
  - `constraint` -> type `decision`, scope `project`
  - `learning_pattern` -> type `lesson`, scope `project`

**Function** `async storeSessionInsights(insights, sessionId, options = {})`:
- For each insight:
  1. Build content string from insight data:
     - decision: `"Decision: {decision}\nReasoning: {reasoning}\nAlternatives: {alternatives}\nConfidence: {confidence}"`
     - reasoning_pattern: `"Reasoning Pattern ({pattern_type}): {description}\nTrigger: {trigger}\nOutcome: {outcome}"`
     - meta_knowledge: `"Meta-Knowledge ({category}): {statement}\nEvidence: {evidence_joined}\nScope: {scope}"`
  2. Check knowledge system availability: `knowledge.isReady(scope)`
  3. If not ready, skip with warning
  4. Use `knowledge.safeAdd()` with:
     - `content`: built content string
     - `type`: mapped type
     - `scope`: mapped scope
     - `ttlCategory`: mapped TTL
     - `metadata`: `{ source: 'haiku_analysis', session_id: sessionId, extraction_type: insight.type, confidence: insight.confidence, context_snippet: insight.context_snippet }`
  5. Track results: `{ stored: N, skipped: N, errors: N }`
- The knowledge system's built-in dedup (three-stage: content hash -> canonical hash -> embedding) handles duplicate prevention automatically
- Return `{ stored, skipped, errors, details: [] }`

**Function** `async analyzeAndStoreSession(sessionFilePath, options = {})`:
- High-level orchestrator for end-to-end analysis
- Steps:
  1. Load session JSONL entries from file (use `require('fs').readFileSync`, parse lines as JSON, handle corrupted lines gracefully)
  2. Run quality gate: `shouldAnalyzeSession(entries)` — if skip, return early with reason
  3. Compute session hash: `computeSessionHash(entries)` — if already analyzed, return early
  4. Run `analyzeSession(entries, options)` from session-analyzer.js
  5. Store insights via `storeSessionInsights(insights, sessionId)`
  6. Mark session as analyzed: `markAnalyzed(sessionHash, sessionId, insights.length)`
  7. Return full results: `{ analyzed: true, sessionId, insightCount, storeResults, analyzerStats }`
- On any error, catch and return `{ analyzed: false, error: errorMessage }` — never throw

**CLI entry point** (when run as `node session-knowledge-writer.js <session-file-path>`):
- Parse argv for session file path
- Call `analyzeAndStoreSession(filePath)`
- Print JSON result to stdout
- Exit with code 0 on success, 1 on error
- This enables the MCP server to spawn it as a detached child process
  </action>
  <verify>
Run `node -e "const w = require('./get-shit-done/bin/session-knowledge-writer.js'); console.log(Object.keys(w)); console.log(typeof w.storeSessionInsights === 'function'); console.log(typeof w.analyzeAndStoreSession === 'function');"` — should output exported keys and true for both function checks.
  </verify>
  <done>Knowledge writer module correctly maps insight types to knowledge entries, uses knowledge system's built-in dedup, provides both granular storeSessionInsights and end-to-end analyzeAndStoreSession, and works as standalone CLI script for detached execution.</done>
</task>

<task type="auto">
  <name>Task 2: Add session-end analysis hook to MCP server</name>
  <files>mcp-servers/telegram-mcp/src/index.ts</files>
  <action>
Modify `mcp-servers/telegram-mcp/src/index.ts` to trigger async Haiku analysis when sessions close.

**Changes to SIGINT/SIGTERM handlers:**

After the existing `await closeSession(currentSessionId)` call, add session analysis trigger:
1. Get session file path: `path.join(process.env.PROJECT_ROOT || path.resolve(__dirname, '../..'), '.planning/telegram-sessions', currentSessionId + '.jsonl')`
2. Check if the file exists (use `existsSync`)
3. If exists, spawn the analysis as a **detached child process** that survives MCP server exit:
   ```typescript
   import { spawn } from 'child_process';
   import { existsSync } from 'fs';

   const analyzerScript = path.resolve(__dirname, '../../get-shit-done/bin/session-knowledge-writer.js');
   if (existsSync(analyzerScript) && existsSync(sessionFilePath)) {
     const child = spawn('node', [analyzerScript, sessionFilePath], {
       detached: true,
       stdio: 'ignore',
       env: { ...process.env, PROJECT_ROOT: projectRoot }
     });
     child.unref(); // Don't wait for analysis to complete
     console.error(`[MCP] Session analysis spawned for ${currentSessionId}`);
   }
   ```
4. This approach ensures:
   - Analysis runs AFTER session data is fully written (closeSession appends session_close entry first)
   - MCP server exits immediately without waiting for Haiku API calls
   - Analysis failure doesn't block or crash shutdown
   - The spawned process inherits ANTHROPIC_API_KEY from environment

**Important:** Add the imports at the top of index.ts: `import { spawn } from 'child_process';` and `import { existsSync } from 'fs';` and `import path from 'path';` (check if path is already imported — it may not be, since the file currently uses only SDK imports).

**Extract shutdown logic** into a helper function `async function shutdown()` to avoid duplicating the analysis trigger in both SIGINT and SIGTERM handlers. Both handlers should call `shutdown()` then `process.exit(0)`.

Do NOT modify the main() function or tool handlers. Only modify the shutdown path.
  </action>
  <verify>
Run `cd /Users/ollorin/get-shit-done/mcp-servers/telegram-mcp && npx tsc --noEmit 2>&1 | head -20` — should compile without errors (or only pre-existing warnings). Check that the shutdown function exists and both SIGINT/SIGTERM handlers use it.
  </verify>
  <done>MCP server triggers detached session analysis on shutdown. Analysis is fire-and-forget (does not block exit). Both SIGINT and SIGTERM handlers share unified shutdown logic. TypeScript compiles without new errors.</done>
</task>

</tasks>

<verification>
1. Knowledge writer module loads: `node -e "require('./get-shit-done/bin/session-knowledge-writer.js'); console.log('OK')"`
2. MCP server TypeScript compiles: `cd mcp-servers/telegram-mcp && npx tsc --noEmit`
3. Insight type mapping is correct (decisions -> decision/long_term, reasoning -> lesson/permanent, preferences -> lesson/global)
4. CLI mode works: `node get-shit-done/bin/session-knowledge-writer.js --help` (or exits gracefully with no args)
5. Shutdown function handles missing files gracefully (no crash if session file doesn't exist)
</verification>

<success_criteria>
End-to-end pipeline operational: MCP server session close triggers detached Haiku analysis which stores validated insights in knowledge system with deduplication. Failures are isolated and logged, never blocking shutdown.
</success_criteria>

<output>
After completion, create `.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-03-SUMMARY.md`
</output>
