---
phase: 11-session-end-knowledge-extraction
plan: 04
type: execute
wave: 4
depends_on: ["11-01", "11-02", "11-03"]
files_modified:
  - get-shit-done/bin/gsd-tools.js
  - get-shit-done/bin/historical-extract.js
  - get-shit-done/workflows/analyze-pending-sessions.md
autonomous: true

must_haves:
  truths:
    - "gsd-tools.js analyze-session command triggers Haiku analysis on a specific session JSONL file"
    - "gsd-tools.js historical-extract command reads completed phases from a project .planning/ directory and spawns sequential Haiku analysis per phase"
    - "gsd-tools.js analysis-status command reports total sessions analyzed, insights extracted, and last analysis timestamp"
    - "gsd-tools.js list-pending-sessions command discovers sessions with session_analysis_pending entries that have not yet been analyzed"
    - "gsd-tools.js store-analysis-result command takes Haiku output JSON and persists insights via knowledge-writer.js storeInsights()"
    - "Historical extraction treats each completed phase as one conversation (conversation_id = phase number) per locked decision #4"
    - "GSD workflow analyze-pending-sessions.md bridges extraction requests to actual Haiku Task() invocations and stores results via gsd-tools.js"
  artifacts:
    - path: "get-shit-done/bin/gsd-tools.js"
      provides: "CLI commands: analyze-session, historical-extract, analysis-status, list-pending-sessions, store-analysis-result"
      contains: "list-pending-sessions"
    - path: "get-shit-done/bin/historical-extract.js"
      provides: "Historical extraction from existing GSD project .planning/ directories"
      contains: "extractFromProject"
    - path: "get-shit-done/workflows/analyze-pending-sessions.md"
      provides: "GSD workflow that reads pending sessions, spawns Haiku Task() subagents, and stores extracted knowledge"
      contains: "Task"
  key_links:
    - from: "get-shit-done/bin/gsd-tools.js"
      to: "get-shit-done/bin/session-analyzer.js"
      via: "require for analyze-session command"
      pattern: "session-analyzer"
    - from: "get-shit-done/bin/gsd-tools.js"
      to: "get-shit-done/bin/historical-extract.js"
      via: "require for historical-extract command"
      pattern: "historical-extract"
    - from: "get-shit-done/bin/gsd-tools.js"
      to: "get-shit-done/bin/knowledge-writer.js"
      via: "require for store-analysis-result command"
      pattern: "knowledge-writer"
    - from: "get-shit-done/workflows/analyze-pending-sessions.md"
      to: "get-shit-done/bin/gsd-tools.js"
      via: "CLI calls to list-pending-sessions and store-analysis-result"
      pattern: "gsd-tools.js"
    - from: "get-shit-done/workflows/analyze-pending-sessions.md"
      to: "Task()"
      via: "Claude Code Task tool with model=haiku for each extraction request"
      pattern: "Task"
    - from: "get-shit-done/bin/historical-extract.js"
      to: "get-shit-done/bin/session-analyzer.js"
      via: "analyzeSession for each phase's content"
      pattern: "analyzeSession"
---

<objective>
Add CLI commands for manual session analysis, historical data mining, analysis status reporting, and the GSD workflow that bridges extraction requests to actual Haiku Task() invocations.

Purpose: Enables manual triggering of analysis (debugging, testing), bulk extraction from historical projects (locked decision #4), visibility into what has been analyzed, and -- critically -- completes the full analysis loop by providing the workflow that reads pending session analysis entries and invokes Haiku via Task() to actually extract knowledge.

Output: Updated gsd-tools.js (5 new subcommands), historical-extract.js (project-level extraction), analyze-pending-sessions.md (GSD workflow for Haiku invocation)
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-CONTEXT.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-01-SUMMARY.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-02-SUMMARY.md
@.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-03-SUMMARY.md
@get-shit-done/bin/gsd-tools.js
@get-shit-done/bin/session-analyzer.js
@get-shit-done/bin/session-quality-gates.js
@get-shit-done/bin/knowledge-writer.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create historical extraction module</name>
  <files>
    get-shit-done/bin/historical-extract.js
  </files>
  <action>
Create historical-extract.js (CommonJS) per locked decision #4:

**`extractFromProject(planningPath)`** - Main entry point. `planningPath` is absolute path to a project's `.planning/` directory:

1. **Discover completed phases:** Read ROADMAP.md from `planningPath/ROADMAP.md`. Parse phase entries. Identify completed phases (marked with `[x]` or having status "Complete" in the progress table). Skip incomplete phases per discretion recommendation.

2. **For each completed phase**, sequentially (not parallel, per discretion recommendation):
   a. Read phase files: ROADMAP.md section for that phase, all *-PLAN.md files, all *-SUMMARY.md files, VERIFICATION.md if exists
   b. Concatenate content into a "session transcript" format:
      - Format as timestamped entries similar to session JSONL: `[0] [phase-start] CONTEXT: Phase {N} goal: {goal}`
      - Each PLAN.md becomes `[{i}] PLAN: {plan objective and tasks}`
      - Each SUMMARY.md becomes `[{i}] COMPLETION: {summary of what was built, decisions made}`
      - VERIFICATION.md becomes `[{i}] VERIFICATION: {results}`
   c. Use `conversation_id = "phase-{phaseNumber}"` per locked decision #4 (treat each phase as one conversation)
   d. Pass formatted content to session-analyzer.js `analyzeSession()` to get extraction requests
   e. **Note:** Like session-analyzer.js, this prepares extraction requests for the calling workflow to execute via Task(). The function returns the prepared requests, not the results.

3. **Return structured result:**
   ```javascript
   {
     projectPath: planningPath,
     phasesFound: number,
     phasesCompleted: number,
     extractionRequests: [
       { phaseNumber: string, conversationId: string, requests: ExtractionRequest[] }
     ]
   }
   ```

**`formatPhaseAsTranscript(phaseFiles)`** - Helper that converts phase files into session-like transcript format.

**`discoverCompletedPhases(roadmapContent)`** - Parse ROADMAP.md to find completed phases. Returns array of `{number, name, goal}`.

Use CommonJS. Use fs/path for file reading. Parse ROADMAP.md with regex patterns matching existing roadmap-parser.js approach.
  </action>
  <verify>
    node -e "const h = require('./get-shit-done/bin/historical-extract.js'); console.log(Object.keys(h)); console.log(typeof h.extractFromProject === 'function');"
  </verify>
  <done>
    historical-extract.js reads completed phases from ROADMAP.md, formats plan/summary files as transcripts, prepares Haiku extraction requests with conversation_id per phase. Sequential processing. Returns structured extraction request arrays for Task() invocation by calling workflow.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add CLI commands to gsd-tools.js</name>
  <files>
    get-shit-done/bin/gsd-tools.js
  </files>
  <action>
Add five new subcommands to gsd-tools.js. Follow existing gsd-tools.js patterns: parse args, lazy-require modules, JSON output.

**1. `analyze-session <session-file-path>`** subcommand:
- Accepts path to a session JSONL file (absolute or relative)
- Lazy-require session-analyzer.js, session-quality-gates.js, session-chunker.js
- Load entries via loadSessionJSONL pattern (read file, parse JSONL lines)
- Run quality gates: if shouldAnalyzeSession returns false, output `{status: "skipped", reason}` and exit
- Check re-analysis: if isAlreadyAnalyzed returns true, output `{status: "already_analyzed"}` and exit
- Prepare session: call prepareSessionForAnalysis for chunking
- Call analyzeSession to get extraction requests
- Output JSON: `{status: "ready", chunkCount, extractionRequests: [...]}`
- The extraction requests are designed for a GSD workflow to pass to Task() subagents

**2. `historical-extract <planning-path>`** subcommand:
- Accepts path to a project's .planning/ directory
- Lazy-require historical-extract.js
- Call extractFromProject(planningPath)
- Output JSON result with phases found, completed, and extraction requests
- If path doesn't exist or has no ROADMAP.md, output error JSON

**3. `analysis-status`** subcommand:
- Lazy-require session-quality-gates.js
- Call getAnalysisStats()
- Output JSON: `{totalAnalyzed, totalInsights, lastAnalysis, analysisLogPath}`
- If no analysis log exists, output `{totalAnalyzed: 0, totalInsights: 0, lastAnalysis: null}`

**4. `list-pending-sessions`** subcommand:
- Scans `.planning/telegram-sessions/*.jsonl` files for entries with `type === 'session_analysis_pending'`
- For each session JSONL file: load all entries, check if any entry has type `session_analysis_pending`, and check whether a corresponding `session_analysis_complete` entry exists AFTER it (if so, skip -- already processed)
- Also check via `isAlreadyAnalyzed()` from session-quality-gates.js as a secondary guard
- Output JSON: `{pending: [{sessionId, sessionPath, extractionRequests, timestamp}], count: number}`
- This is the entry point for the analyze-pending-sessions GSD workflow to discover work

**5. `store-analysis-result <session-id> <results-json>`** subcommand:
- Accepts a session ID and a JSON string (or path to JSON file) containing Haiku extraction results
- Lazy-require knowledge-writer.js, session-quality-gates.js, session-analyzer.js
- Parse the results JSON: expects `{type: string, result: string}[]` where each result is Haiku's raw JSON output for one extraction type
- For each result, call `parseExtractionResult(type, result)` from session-analyzer.js to validate and extract insights
- Call `storeInsights(allInsights, {sessionId, scope: 'project'})` from knowledge-writer.js to persist
- Call `markSessionAnalyzed(sessionId, contentHash, insightCount)` from session-quality-gates.js to prevent re-analysis
- Append `{type: 'session_analysis_complete', insight_count, timestamp}` entry to the session JSONL file
- Output JSON: `{stored, skipped, evolved, errors}`
- This is called by the analyze-pending-sessions GSD workflow after Haiku Task() returns

Add all five to the command help text at the top of gsd-tools.js in the appropriate section. Add them to the command dispatch switch/if chain following existing patterns.

Also add to the Usage comment block:
```
 * Session Analysis:
 *   analyze-session <path>           Prepare session for Haiku analysis
 *   historical-extract <path>        Extract knowledge from existing project
 *   analysis-status                  Show analysis statistics
 *   list-pending-sessions            Find sessions awaiting Haiku analysis
 *   store-analysis-result <id> <json> Store Haiku extraction results
```
  </action>
  <verify>
    node /Users/ollorin/get-shit-done/get-shit-done/bin/gsd-tools.js analysis-status 2>&1
  </verify>
  <done>
    gsd-tools.js has five new subcommands: analyze-session (prepares session for analysis), historical-extract (prepares historical project for analysis), analysis-status (reports analysis statistics), list-pending-sessions (discovers pending sessions for workflow), store-analysis-result (persists Haiku output to knowledge DB). All output JSON. All follow existing gsd-tools patterns. Help text updated.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create GSD workflow for pending session analysis</name>
  <files>
    get-shit-done/workflows/analyze-pending-sessions.md
  </files>
  <action>
Create a new GSD workflow file at `~/.claude/get-shit-done/workflows/analyze-pending-sessions.md`. This workflow is the CRITICAL bridge that completes the full analysis loop: session -> extraction requests -> Haiku Task() -> knowledge storage.

**This workflow is invoked manually** (e.g., `/gsd:analyze-pending-sessions`) or by a Claude Code hook at session start. It reads pending sessions, spawns Haiku subagents, and stores results.

The workflow file should follow the existing GSD workflow pattern (see other .md files in get-shit-done/workflows/) and contain:

**1. Purpose section:**
Explain that this workflow processes sessions marked as `session_analysis_pending` by the Telegram MCP server. It spawns Haiku subagents to extract knowledge (decisions, reasoning patterns, meta-knowledge) and stores the results in the project's knowledge database.

**2. Process section with steps:**

**Step 1: Discover pending sessions**
```bash
PENDING=$(node /Users/ollorin/.claude/get-shit-done/bin/gsd-tools.js list-pending-sessions)
```
Parse the JSON output. If `count === 0`, output "No pending sessions to analyze" and exit.

**Step 2: For each pending session**, process sequentially:
  a. Read the session's extraction requests from the PENDING output (each has `{sessionId, sessionPath, extractionRequests}`)
  b. For each extraction request (typically 3: decision, reasoning, meta-knowledge):
     - Spawn a Haiku subagent using Claude Code's Task tool: `Task(subagent_type="general-purpose", model="haiku", prompt=extractionRequest.prompt)`
     - Collect the Haiku output (raw JSON text)
  c. Assemble all results into a JSON array: `[{type: "decision", result: haikuOutput1}, {type: "reasoning_pattern", result: haikuOutput2}, ...]`
  d. Call gsd-tools.js to store the results:
     ```bash
     node /Users/ollorin/.claude/get-shit-done/bin/gsd-tools.js store-analysis-result "<sessionId>" '<results-json>'
     ```
  e. Log result (stored/skipped/errors)

**Step 3: Summary**
After processing all pending sessions, output a summary: total sessions processed, total insights stored, any errors.

**3. Key constraints (document in workflow):**
- ZERO direct API calls -- all Haiku work via Task(subagent_type="general-purpose", model="haiku")
- Sequential processing (one session at a time) to avoid overwhelming the system
- If a Haiku Task() fails for one extraction type, continue with the others (partial analysis is better than none)
- If store-analysis-result fails, log the error but continue with next session

**4. Invocation guidance:**
Document that this workflow can be invoked:
- Manually: `/gsd:analyze-pending-sessions`
- Automatically: Via a Claude Code SessionStart hook that checks for pending sessions
- After historical-extract: Run this workflow to process the extraction requests generated by historical-extract

The workflow should be written as a Markdown file with `<purpose>`, `<process>`, and `<step>` tags following the existing GSD workflow patterns visible in the workflows directory.
  </action>
  <verify>
    test -f /Users/ollorin/.claude/get-shit-done/workflows/analyze-pending-sessions.md && echo "Workflow file exists" || echo "MISSING"
  </verify>
  <done>
    GSD workflow file exists at get-shit-done/workflows/analyze-pending-sessions.md. It reads pending sessions via list-pending-sessions, spawns Haiku Task() subagents for each extraction type, and stores results via store-analysis-result. The full analysis loop is complete: session_analysis_pending -> Haiku Task() -> storeInsights -> session_analysis_complete.
  </done>
</task>

</tasks>

<verification>
1. `node gsd-tools.js analyze-session /path/to/nonexistent.jsonl` returns error JSON gracefully
2. `node gsd-tools.js historical-extract /path/to/nonexistent/.planning` returns error JSON gracefully
3. `node gsd-tools.js analysis-status` returns JSON with totalAnalyzed field
4. `node gsd-tools.js list-pending-sessions` returns JSON with pending array and count field
5. `node gsd-tools.js store-analysis-result test-id '[]'` handles empty results gracefully
6. `node -e "require('./get-shit-done/bin/historical-extract.js')"` loads without error
7. No imports of `@anthropic-ai/sdk` in any new or modified files
8. historical-extract uses conversation_id per phase (locked decision #4)
9. analyze-pending-sessions.md workflow file exists and references Task(model="haiku")
10. Full loop verified: list-pending-sessions -> workflow reads pending -> Task() for extraction -> store-analysis-result persists
</verification>

<success_criteria>
- Manual analysis via CLI: `gsd-tools.js analyze-session` prepares any session file for analysis
- Historical mining: `gsd-tools.js historical-extract` reads completed phases and prepares extraction requests
- Status reporting: `gsd-tools.js analysis-status` shows what has been analyzed
- Pending session discovery: `gsd-tools.js list-pending-sessions` finds sessions awaiting analysis
- Result storage: `gsd-tools.js store-analysis-result` persists Haiku output to knowledge DB and marks session analyzed
- Full analysis loop complete: session_analysis_pending -> GSD workflow -> Haiku Task() -> storeInsights -> session_analysis_complete
- All analysis uses subscription-only Task() pattern (extraction requests returned for workflow invocation)
- Sequential historical processing (not parallel) per discretion recommendation
</success_criteria>

<output>
After completion, create `.planning/phases/11-session-end-knowledge-extraction-implement-haiku-based-analysis-of-completed-sessions-to-extract-reasoning-patterns-and-decisions-beyond-keyword-matching/11-04-SUMMARY.md`
</output>
