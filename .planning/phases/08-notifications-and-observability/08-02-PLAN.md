---
phase: 08-notifications-and-observability
plan: 02
type: execute
wave: 2
depends_on: ["08-01"]
files_modified:
  - get-shit-done/bin/whisper-transcribe.js
  - get-shit-done/bin/telegram-bot.js
autonomous: true
user_setup:
  - service: ffmpeg
    why: "Audio format conversion for Whisper"
    env_vars: []
    dashboard_config:
      - task: "Install ffmpeg"
        location: "macOS: brew install ffmpeg | Linux: apt install ffmpeg"
  - service: whisper
    why: "Local speech-to-text model"
    env_vars: []
    dashboard_config:
      - task: "Download Whisper model after npm install"
        location: "Run: npx whisper-node download"

must_haves:
  truths:
    - "User can send voice messages via Telegram"
    - "Voice messages are transcribed locally via Whisper"
    - "Transcribed text is returned to Claude as if user typed it"
    - "System handles transcription errors gracefully"
  artifacts:
    - path: "get-shit-done/bin/whisper-transcribe.js"
      provides: "Audio download, conversion, and transcription pipeline"
      exports: ["transcribeAudio", "downloadFile", "convertToWav"]
    - path: "get-shit-done/bin/telegram-bot.js"
      provides: "Voice message handler"
      contains: "bot.on.*voice"
  key_links:
    - from: "telegram-bot.js"
      to: "whisper-transcribe.js"
      via: "voice handler imports transcription"
      pattern: "require.*whisper-transcribe"
    - from: "whisper-transcribe.js"
      to: "ffmpeg binary"
      via: "fluent-ffmpeg for audio conversion"
      pattern: "fluent-ffmpeg"
---

<objective>
Add voice message support to Telegram bot with local Whisper transcription

Purpose: Allow users to respond to blocking questions via voice messages when typing is inconvenient (mobile, driving, etc.). Voice is transcribed locally for privacy and zero API cost.

Output: Voice message handler that downloads Telegram audio, converts via ffmpeg, transcribes via whisper-node, and returns text to conversation flow.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-notifications-and-observability/08-RESEARCH.md
@.planning/phases/08-notifications-and-observability/08-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create whisper-transcribe.js module</name>
  <files>get-shit-done/bin/whisper-transcribe.js</files>
  <action>
Create local audio transcription pipeline:

1. Install dependencies:
   ```bash
   cd get-shit-done && npm install whisper-node fluent-ffmpeg
   ```

2. Create module with:

```javascript
const whisper = require('whisper-node');
const ffmpeg = require('fluent-ffmpeg');
const fs = require('fs').promises;
const https = require('https');
const path = require('path');
const os = require('os');

const WHISPER_MODEL = 'base.en'; // 244M params, best accuracy/speed balance

/**
 * Download file from URL to local path
 */
async function downloadFile(url, destPath) {
  return new Promise((resolve, reject) => {
    const file = require('fs').createWriteStream(destPath);
    https.get(url, (response) => {
      // Handle redirects
      if (response.statusCode >= 300 && response.statusCode < 400 && response.headers.location) {
        return downloadFile(response.headers.location, destPath).then(resolve).catch(reject);
      }
      response.pipe(file);
      file.on('finish', () => file.close(resolve));
    }).on('error', (err) => {
      fs.unlink(destPath).catch(() => {});
      reject(err);
    });
  });
}

/**
 * Convert audio file to 16kHz mono WAV (Whisper requirement)
 */
async function convertToWav(inputPath, outputPath) {
  return new Promise((resolve, reject) => {
    ffmpeg(inputPath)
      .audioFrequency(16000)
      .audioChannels(1)
      .format('wav')
      .on('end', resolve)
      .on('error', reject)
      .save(outputPath);
  });
}

/**
 * Transcribe audio file using local Whisper model
 * @param {string} audioUrl - URL to download audio from
 * @returns {string} - Transcribed text
 */
async function transcribeAudio(audioUrl) {
  const tempDir = os.tmpdir();
  const timestamp = Date.now();
  const tempOga = path.join(tempDir, `telegram_${timestamp}.oga`);
  const tempWav = path.join(tempDir, `telegram_${timestamp}.wav`);

  try {
    // 1. Download audio file
    await downloadFile(audioUrl, tempOga);

    // 2. Convert to 16kHz mono WAV
    await convertToWav(tempOga, tempWav);

    // 3. Transcribe with Whisper
    const transcript = await whisper(tempWav, {
      modelName: WHISPER_MODEL,
      whisperOptions: {
        language: 'en',
        word_timestamps: false
      }
    });

    // Join speech segments
    const text = transcript.map(t => t.speech).join(' ').trim();

    return text || '[No speech detected]';
  } catch (error) {
    console.error('Transcription error:', error.message);
    throw new Error(`Transcription failed: ${error.message}`);
  } finally {
    // Cleanup temp files
    await fs.unlink(tempOga).catch(() => {});
    await fs.unlink(tempWav).catch(() => {});
  }
}

/**
 * Check if Whisper model is available
 */
async function checkWhisperModel() {
  const modelPaths = [
    path.join(os.homedir(), '.cache', 'whisper', `ggml-${WHISPER_MODEL}.bin`),
    path.join(__dirname, 'node_modules', 'whisper-node', 'lib', 'whisper.cpp', 'models', `ggml-${WHISPER_MODEL}.bin`)
  ];

  for (const modelPath of modelPaths) {
    try {
      await fs.access(modelPath);
      return { available: true, path: modelPath };
    } catch {}
  }

  return {
    available: false,
    message: 'Whisper model not found. Run: npx whisper-node download'
  };
}

module.exports = {
  transcribeAudio,
  downloadFile,
  convertToWav,
  checkWhisperModel,
  WHISPER_MODEL
};
```

Use Pattern 2 from 08-RESEARCH.md as reference.
  </action>
  <verify>
```bash
node -e "const wt = require('./get-shit-done/bin/whisper-transcribe.js'); console.log(typeof wt.transcribeAudio, typeof wt.checkWhisperModel)"
```
Output: `function function`
  </verify>
  <done>Module exports transcription functions, handles errors gracefully</done>
</task>

<task type="auto">
  <name>Task 2: Add voice message handler to telegram-bot.js</name>
  <files>get-shit-done/bin/telegram-bot.js</files>
  <action>
Add voice message handling to existing telegram-bot.js:

1. Add require at top:
   ```javascript
   const { transcribeAudio, checkWhisperModel } = require('./whisper-transcribe.js');
   ```

2. Add voice message handler after text handler:
   ```javascript
   bot.on('voice', async (ctx) => {
     const conversation = require('./telegram-conversation.js');
     const pending = conversation.getPendingQuestions();

     if (pending.length === 0) {
       await ctx.reply('No pending question. Voice message ignored.');
       return;
     }

     // Get most recent pending question
     const latestQuestion = pending[pending.length - 1];

     try {
       // Check model availability
       const modelStatus = await checkWhisperModel();
       if (!modelStatus.available) {
         await ctx.reply(`Cannot process voice: ${modelStatus.message}`);
         return;
       }

       // Notify user we're processing
       await ctx.reply('Processing voice message...');

       // Get file download URL
       const fileId = ctx.message.voice.file_id;
       const fileSize = ctx.message.voice.file_size;

       // Check file size limit (20MB)
       if (fileSize > 20 * 1024 * 1024) {
         await ctx.reply('Voice message too large (>20MB). Please send a shorter clip.');
         return;
       }

       const fileLink = await ctx.telegram.getFileLink(fileId);

       // Transcribe
       const transcription = await transcribeAudio(fileLink.href);

       // Confirm transcription to user
       await ctx.reply(`Transcribed: "${transcription}"\n\nResuming execution...`);

       // Resolve the pending question
       conversation.handleResponse(latestQuestion.questionId, {
         type: 'voice',
         content: transcription,
         original_duration: ctx.message.voice.duration
       });
     } catch (error) {
       await ctx.reply(`Transcription error: ${error.message}`);
     }
   });
   ```

3. Add audio handler for non-voice audio files (same logic):
   ```javascript
   bot.on('audio', async (ctx) => {
     // Same pattern as voice handler
     // ... (copy voice handler logic)
   });
   ```

4. Add `/whisper` command to check model status:
   ```javascript
   bot.command('whisper', async (ctx) => {
     const status = await checkWhisperModel();
     if (status.available) {
       await ctx.reply(`Whisper model ready: ${status.path}`);
     } else {
       await ctx.reply(`Whisper not available: ${status.message}`);
     }
   });
   ```
  </action>
  <verify>
```bash
grep -c "bot.on.*voice" get-shit-done/bin/telegram-bot.js
```
Output: `1` or more
  </verify>
  <done>Voice message handler added, transcription integrated with conversation flow</done>
</task>

<task type="auto">
  <name>Task 3: Verify voice transcription module integration</name>
  <files>get-shit-done/bin/whisper-transcribe.js, get-shit-done/bin/telegram-bot.js</files>
  <action>
Verify all voice transcription components work correctly without requiring Whisper model or Telegram:

1. Test whisper-transcribe module loading and exports:
   ```bash
   node -e "
     const wt = require('./get-shit-done/bin/whisper-transcribe.js');
     console.log('transcribeAudio:', typeof wt.transcribeAudio);
     console.log('downloadFile:', typeof wt.downloadFile);
     console.log('convertToWav:', typeof wt.convertToWav);
     console.log('checkWhisperModel:', typeof wt.checkWhisperModel);
     console.log('WHISPER_MODEL:', wt.WHISPER_MODEL);
   "
   ```
   Expected: All functions exported, WHISPER_MODEL is 'base.en'

2. Test checkWhisperModel returns correct structure:
   ```bash
   node -e "
     const wt = require('./get-shit-done/bin/whisper-transcribe.js');
     wt.checkWhisperModel().then(status => {
       console.log('Has available property:', 'available' in status);
       if (!status.available) console.log('Has message:', 'message' in status);
       if (status.available) console.log('Has path:', 'path' in status);
     });
   "
   ```
   Expected: `Has available property: true` and either message or path property

3. Test telegram-bot.js has voice handler:
   ```bash
   grep -E "bot\\.on\\(['\"]voice['\"]" get-shit-done/bin/telegram-bot.js
   ```
   Expected: Line containing `bot.on('voice'` or `bot.on("voice"`

4. Test telegram-bot.js imports whisper-transcribe:
   ```bash
   grep -E "require.*whisper-transcribe" get-shit-done/bin/telegram-bot.js
   ```
   Expected: Line showing the require statement

5. Test fluent-ffmpeg is installed:
   ```bash
   node -e "require('fluent-ffmpeg'); console.log('fluent-ffmpeg: installed')"
   ```
   Expected: `fluent-ffmpeg: installed`

All tests verify module structure and integration without requiring actual audio processing.
  </action>
  <verify>
```bash
# All voice modules properly integrated
node -e "const wt = require('./get-shit-done/bin/whisper-transcribe.js'); console.log('Exports:', Object.keys(wt).length >= 4)"
```
Output: `Exports: true`
  </verify>
  <done>Whisper transcribe module exports all functions, telegram-bot.js has voice handler, dependencies installed</done>
</task>

</tasks>

<verification>
Phase verification criteria:
1. `node -e "require('./get-shit-done/bin/whisper-transcribe.js').checkWhisperModel().then(console.log)"` shows model status
2. Voice message sent to Telegram bot is transcribed and returned as text response
3. File size limit (20MB) is enforced
4. Transcription errors are handled gracefully with user feedback
</verification>

<success_criteria>
- whisper-transcribe.js created with transcription pipeline
- telegram-bot.js handles voice messages
- Voice -> text conversion works end-to-end
- Graceful degradation when Whisper model not installed
</success_criteria>

<output>
After completion, create `.planning/phases/08-notifications-and-observability/08-02-SUMMARY.md`
</output>
