---
phase: 08-notifications-and-observability
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/observability.js
  - get-shit-done/bin/llm-metrics.js
  - get-shit-done/bin/gsd-tools.js
autonomous: true

must_haves:
  truths:
    - "Multi-agent workflows produce distributed traces with span-level detail"
    - "LLM calls capture tokens, cost, context size, and latency metrics"
    - "Traces can be exported to OTLP-compatible backends"
    - "System works without tracing backend (graceful degradation)"
  artifacts:
    - path: "get-shit-done/bin/observability.js"
      provides: "OpenTelemetry SDK initialization and tracer access"
      exports: ["initTracing", "getTracer", "shutdownTracing"]
    - path: "get-shit-done/bin/llm-metrics.js"
      provides: "LLM-specific metric collection with gen_ai.* attributes"
      exports: ["createLLMSpan", "recordLLMUsage", "calculateCost"]
    - path: "get-shit-done/bin/gsd-tools.js"
      provides: "observability commands"
      contains: "observability"
  key_links:
    - from: "llm-metrics.js"
      to: "observability.js"
      via: "imports tracer for span creation"
      pattern: "require.*observability"
    - from: "observability.js"
      to: "@opentelemetry/sdk-node"
      via: "NodeSDK initialization"
      pattern: "NodeSDK"
---

<objective>
Implement distributed tracing with OpenTelemetry for multi-agent workflow observability

Purpose: Track autonomous roadmap execution across phases, sub-coordinators, and LLM calls with span-level detail. Capture LLM-specific metrics (tokens, cost, context size, latency) for cost control and performance analysis.

Output: OpenTelemetry instrumentation that captures traces for all LLM operations with gen_ai.* semantic conventions.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/08-notifications-and-observability/08-RESEARCH.md
@.planning/phases/06-autonomous-execution-core/06-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create observability.js module</name>
  <files>get-shit-done/bin/observability.js</files>
  <action>
Install OpenTelemetry dependencies and create tracing initialization module:

1. Install dependencies:
   ```bash
   cd get-shit-done && npm install @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node @opentelemetry/exporter-otlp-grpc @opentelemetry/resources @opentelemetry/semantic-conventions
   ```

2. Create observability.js:

```javascript
/**
 * OpenTelemetry Initialization for GSD Autonomous Execution
 *
 * Provides distributed tracing across multi-agent workflows.
 * Pattern: OpenTelemetry with gen_ai.* semantic conventions
 * Source: Phase 8 Research - Pattern 3
 */

const { NodeSDK } = require('@opentelemetry/sdk-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-otlp-grpc');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
const { trace } = require('@opentelemetry/api');

let sdk = null;
let isInitialized = false;

/**
 * Initialize OpenTelemetry tracing
 * @param {object} options - Configuration options
 * @param {string} options.serviceName - Service name for traces (default: 'gsd-autonomous-execution')
 * @param {string} options.endpoint - OTLP endpoint (default: env.OTEL_EXPORTER_OTLP_ENDPOINT or disabled)
 */
function initTracing(options = {}) {
  if (isInitialized) {
    console.log('OpenTelemetry already initialized');
    return;
  }

  const serviceName = options.serviceName || 'gsd-autonomous-execution';
  const endpoint = options.endpoint || process.env.OTEL_EXPORTER_OTLP_ENDPOINT;

  // If no endpoint configured, create no-op tracer
  if (!endpoint) {
    console.log('OTEL_EXPORTER_OTLP_ENDPOINT not set. Tracing disabled (no-op mode).');
    isInitialized = true;
    return;
  }

  try {
    sdk = new NodeSDK({
      resource: new Resource({
        [SemanticResourceAttributes.SERVICE_NAME]: serviceName,
        [SemanticResourceAttributes.SERVICE_VERSION]: '1.0.0',
      }),
      traceExporter: new OTLPTraceExporter({
        url: endpoint,
      }),
    });

    sdk.start();
    isInitialized = true;
    console.log(`OpenTelemetry tracing enabled. Exporting to: ${endpoint}`);
  } catch (error) {
    console.error('Failed to initialize OpenTelemetry:', error.message);
    isInitialized = true; // Mark as initialized to prevent retry loops
  }
}

/**
 * Get tracer for creating spans
 * @param {string} tracerName - Name of the tracer (default: 'gsd')
 */
function getTracer(tracerName = 'gsd') {
  return trace.getTracer(tracerName);
}

/**
 * Graceful shutdown of tracing
 */
async function shutdownTracing() {
  if (sdk) {
    try {
      await sdk.shutdown();
      console.log('OpenTelemetry tracing terminated');
    } catch (error) {
      console.error('Error shutting down OpenTelemetry:', error.message);
    }
  }
}

/**
 * Get current span context for propagation to sub-coordinators
 * Returns traceparent header value for W3C Trace Context
 */
function getTraceContext() {
  const currentSpan = trace.getActiveSpan();
  if (!currentSpan) return null;

  const ctx = currentSpan.spanContext();
  // W3C Trace Context format: version-traceId-spanId-flags
  return `00-${ctx.traceId}-${ctx.spanId}-01`;
}

/**
 * Check if tracing is enabled
 */
function isTracingEnabled() {
  return isInitialized && sdk !== null;
}

module.exports = {
  initTracing,
  getTracer,
  shutdownTracing,
  getTraceContext,
  isTracingEnabled
};
```
  </action>
  <verify>
```bash
node -e "const obs = require('./get-shit-done/bin/observability.js'); console.log(typeof obs.initTracing, typeof obs.getTracer)"
```
Output: `function function`
  </verify>
  <done>OpenTelemetry initialization module created, graceful no-op when endpoint not configured</done>
</task>

<task type="auto">
  <name>Task 2: Create llm-metrics.js for LLM-specific instrumentation</name>
  <files>get-shit-done/bin/llm-metrics.js</files>
  <action>
Create LLM metrics collection module with gen_ai.* semantic conventions:

```javascript
/**
 * LLM-Specific Metrics Collection
 *
 * Captures tokens, cost, context size, and latency per LLM operation.
 * Uses OpenTelemetry gen_ai.* semantic conventions.
 * Source: Phase 8 Research - Pattern 3
 */

const { SpanStatusCode } = require('@opentelemetry/api');
const { getTracer, isTracingEnabled } = require('./observability.js');

// Claude pricing (per 1M tokens)
const CLAUDE_PRICING = {
  'claude-opus-4-5': { input: 5, output: 25, cached: 0.5 },
  'claude-sonnet-4': { input: 3, output: 15, cached: 0.3 },
  'claude-haiku-3-5': { input: 1, output: 5, cached: 0.1 },
  // Aliases
  'opus': { input: 5, output: 25, cached: 0.5 },
  'sonnet': { input: 3, output: 15, cached: 0.3 },
  'haiku': { input: 1, output: 5, cached: 0.1 }
};

/**
 * Calculate cost for an LLM operation
 * @param {string} model - Model name
 * @param {object} usage - Token usage { input_tokens, output_tokens, cache_read_input_tokens }
 * @returns {object} - Cost breakdown { input_cost, output_cost, total_cost }
 */
function calculateCost(model, usage) {
  const pricing = CLAUDE_PRICING[model] || CLAUDE_PRICING['opus'];

  const baseInputTokens = usage.input_tokens - (usage.cache_read_input_tokens || 0);
  const cachedTokens = usage.cache_read_input_tokens || 0;

  const inputCost = (baseInputTokens / 1000000) * pricing.input;
  const cachedCost = (cachedTokens / 1000000) * pricing.cached;
  const outputCost = (usage.output_tokens / 1000000) * pricing.output;

  return {
    input_cost: inputCost + cachedCost,
    output_cost: outputCost,
    total_cost: inputCost + cachedCost + outputCost
  };
}

/**
 * Create an LLM span with gen_ai.* attributes
 * @param {string} operation - Operation name (e.g., 'research', 'plan', 'execute')
 * @param {object} metadata - Additional context
 * @returns {object} - Span wrapper with end() method
 */
function createLLMSpan(operation, metadata = {}) {
  if (!isTracingEnabled()) {
    // Return no-op wrapper
    return {
      setAttributes: () => {},
      recordException: () => {},
      setStatus: () => {},
      end: () => {},
      isNoOp: true
    };
  }

  const tracer = getTracer('gsd-llm');
  const span = tracer.startSpan(`llm.${operation}`);

  // Set initial gen_ai.* attributes
  span.setAttributes({
    'gen_ai.system': 'anthropic',
    'gen_ai.operation.name': operation,
    'gen_ai.request.model': metadata.model || 'unknown',
    'gen_ai.request.max_tokens': metadata.max_tokens || 200000,
    'gen_ai.request.temperature': metadata.temperature || 1.0,
    'gsd.phase': metadata.phase || 'unknown',
    'gsd.plan': metadata.plan || 'unknown'
  });

  return {
    setAttributes: (attrs) => span.setAttributes(attrs),
    recordException: (error) => span.recordException(error),
    setStatus: (code, message) => span.setStatus({ code, message }),
    end: () => span.end(),
    _span: span
  };
}

/**
 * Record LLM usage metrics on a span
 * @param {object} spanWrapper - Span from createLLMSpan
 * @param {object} response - Claude API response with usage field
 */
function recordLLMUsage(spanWrapper, response) {
  if (spanWrapper.isNoOp) return;

  const usage = response.usage || {};
  const model = response.model || 'unknown';
  const cost = calculateCost(model, usage);

  spanWrapper.setAttributes({
    'gen_ai.response.model': model,
    'gen_ai.usage.input_tokens': usage.input_tokens || 0,
    'gen_ai.usage.output_tokens': usage.output_tokens || 0,
    'gen_ai.usage.total_tokens': (usage.input_tokens || 0) + (usage.output_tokens || 0),
    'gen_ai.usage.cache_read_tokens': usage.cache_read_input_tokens || 0,
    'gsd.cost.input_usd': cost.input_cost,
    'gsd.cost.output_usd': cost.output_cost,
    'gsd.cost.total_usd': cost.total_cost
  });

  spanWrapper.setStatus(SpanStatusCode.OK);
}

/**
 * Record an error on an LLM span
 * @param {object} spanWrapper - Span from createLLMSpan
 * @param {Error} error - The error that occurred
 */
function recordLLMError(spanWrapper, error) {
  if (spanWrapper.isNoOp) return;

  spanWrapper.recordException(error);
  spanWrapper.setStatus(SpanStatusCode.ERROR, error.message);
}

/**
 * Wrap an LLM call with tracing
 * @param {string} operation - Operation name
 * @param {object} metadata - Context metadata
 * @param {function} fn - Async function to execute
 */
async function withLLMTracing(operation, metadata, fn) {
  const span = createLLMSpan(operation, metadata);
  const startTime = Date.now();

  try {
    const result = await fn();

    // Record timing
    span.setAttributes({
      'gsd.duration_ms': Date.now() - startTime
    });

    // If result has usage, record it
    if (result && result.usage) {
      recordLLMUsage(span, result);
    }

    return result;
  } catch (error) {
    recordLLMError(span, error);
    throw error;
  } finally {
    span.end();
  }
}

module.exports = {
  calculateCost,
  createLLMSpan,
  recordLLMUsage,
  recordLLMError,
  withLLMTracing,
  CLAUDE_PRICING
};
```
  </action>
  <verify>
```bash
node -e "const llm = require('./get-shit-done/bin/llm-metrics.js'); const cost = llm.calculateCost('opus', {input_tokens: 1000, output_tokens: 500}); console.log(cost.total_cost > 0)"
```
Output: `true`
  </verify>
  <done>LLM metrics module captures tokens, cost, and timing with gen_ai.* attributes</done>
</task>

<task type="auto">
  <name>Task 3: Add observability CLI commands to gsd-tools.js</name>
  <files>get-shit-done/bin/gsd-tools.js</files>
  <action>
Add observability command handler to gsd-tools.js:

1. Create cmdObservability handler with subcommands:
   - `observability init` - initialize tracing (useful for testing)
   - `observability status` - show tracing status (enabled/disabled, endpoint)
   - `observability cost <model> <input> <output> [--cached <cached>]` - calculate cost
   - `observability shutdown` - graceful shutdown

2. Add to command router:
   ```javascript
   case 'observability':
     await cmdObservability(args.slice(1));
     break;
   ```

3. Add to help text:
   ```
   observability init|status|cost|shutdown  OpenTelemetry tracing operations
   ```

4. Implementation:
   ```javascript
   async function cmdObservability(args) {
     const subcommand = args[0];

     switch (subcommand) {
       case 'init':
         const obs = require('./observability.js');
         obs.initTracing({ serviceName: 'gsd-cli' });
         break;

       case 'status':
         const obs = require('./observability.js');
         console.log(JSON.stringify({
           enabled: obs.isTracingEnabled(),
           endpoint: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'not configured'
         }, null, 2));
         break;

       case 'cost':
         const llm = require('./llm-metrics.js');
         const model = args[1] || 'opus';
         const input = parseInt(args[2]) || 0;
         const output = parseInt(args[3]) || 0;
         const cachedIdx = args.indexOf('--cached');
         const cached = cachedIdx > -1 ? parseInt(args[cachedIdx + 1]) : 0;

         const cost = llm.calculateCost(model, {
           input_tokens: input,
           output_tokens: output,
           cache_read_input_tokens: cached
         });

         console.log(JSON.stringify({
           model,
           input_tokens: input,
           output_tokens: output,
           cached_tokens: cached,
           ...cost
         }, null, 2));
         break;

       case 'shutdown':
         const obs = require('./observability.js');
         await obs.shutdownTracing();
         break;

       default:
         console.log('Usage: observability init|status|cost|shutdown');
     }
   }
   ```
  </action>
  <verify>
```bash
node get-shit-done/bin/gsd-tools.js observability status
```
Output contains: `"enabled":` and `"endpoint":`
  </verify>
  <done>Observability CLI commands added with cost calculation and status check</done>
</task>

</tasks>

<verification>
Phase verification criteria:
1. `node get-shit-done/bin/gsd-tools.js observability status` shows disabled when no endpoint
2. `node get-shit-done/bin/gsd-tools.js observability cost opus 10000 5000` calculates correct cost
3. Modules load without error when OpenTelemetry dependencies missing (graceful degradation)
</verification>

<success_criteria>
- observability.js provides OpenTelemetry initialization with graceful no-op mode
- llm-metrics.js captures tokens, cost, and gen_ai.* attributes
- gsd-tools.js has observability commands with 4 subcommands
- Cost calculation handles prompt caching correctly
</success_criteria>

<output>
After completion, create `.planning/phases/08-notifications-and-observability/08-03-SUMMARY.md`
</output>
