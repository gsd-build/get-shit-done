---
phase: 07-autonomous-execution-optimization
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/task-chunker.js
  - get-shit-done/bin/gsd-tools.js
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Large task detection identifies work exceeding single context capacity"
    - "Task chunking splits large tasks into batches"
    - "Batch processing optimizes repetitive operations"
  artifacts:
    - path: "get-shit-done/bin/task-chunker.js"
      provides: "TaskChunker class with analyzeTask and createChunks methods"
      min_lines: 150
      exports: ["TaskChunker", "analyzeTask", "estimateTaskTokens"]
    - path: "get-shit-done/bin/gsd-tools.js"
      provides: "CLI commands: task analyze, task chunk, task batch"
      contains: "cmdTask"
  key_links:
    - from: "gsd-tools.js"
      to: "task-chunker.js"
      via: "require('./task-chunker.js')"
      pattern: "require.*task-chunker"
    - from: "task-chunker.js"
      to: "token estimation"
      via: "estimateTaskTokens function"
      pattern: "estimateTask(Tokens|Complexity)"
---

<objective>
Close gaps for EXEC-19 (large task detection), EXEC-20 (task chunking), and EXEC-22 (batch processing).

Purpose: Enable autonomous execution to handle large tasks (e.g., "update 350 tests") by detecting oversized work, splitting into manageable batches, and optimizing repetitive operations.
Output: TaskChunker module with CLI integration
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-autonomous-execution-optimization/07-VERIFICATION.md
@.planning/phases/07-autonomous-execution-optimization/07-RESEARCH.md
@get-shit-done/bin/token-monitor.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create task-chunker.js module</name>
  <files>get-shit-done/bin/task-chunker.js</files>
  <action>
Create TaskChunker class following Pattern 3 from 07-RESEARCH.md:

1. **Constructor:**
   - `contextLimit` parameter (default 200000)
   - `safeContextUsage` = contextLimit * 0.7 (140k tokens threshold)
   - Constants for chunk strategies: 'batch', 'file-batch', 'recursive-search', 'semantic'

2. **estimateTaskTokens(task):**
   Multi-signal heuristics for task complexity:
   - Base tokens: 5000 (task setup)
   - File operations: 500 tokens per file
   - Description complexity: 1 token per char
   - Repetitive operations: detect via regex `/update.*tests?|migrate|refactor|rename|replace/i`
   - Dependencies: 2000 tokens per dependency
   - Search operations: detect via `/find|search|grep|scan/i`, add 10000 tokens
   - Return: { tokens, signals, complexity ('simple'|'moderate'|'complex'|'very-complex') }

3. **analyzeTask(task):**
   - Call estimateTaskTokens to get estimate
   - Compare against safeContextUsage (140k)
   - If needsChunking=false: return { needsChunking: false, estimatedTokens, chunkCount: 1, strategy: 'single-pass' }
   - If needsChunking=true: determine strategy, calculate chunkCount, call createChunks
   - Return: { needsChunking, estimatedTokens, chunkCount, strategy, chunks? }

4. **selectChunkingStrategy(task, estimate):**
   - If estimate.signals.isRepetitive: return 'batch'
   - If estimate.signals.requiresSearch: return 'recursive-search'
   - If task.files?.length > 50: return 'file-batch'
   - Default: return 'semantic'

5. **createChunks(task, strategy, chunkCount):**
   Switch on strategy:
   - 'batch': createBatchChunks - split operation count evenly (e.g., 350 tests -> 10 batches of 35)
   - 'file-batch': createFileChunks - split files evenly
   - 'recursive-search': createRecursiveChunks - search -> analyze -> execute stages
   - 'semantic': createSemanticChunks - divide into parts

6. **extractOperationCount(description):**
   - Extract number from patterns like "update 350 tests", "rename 50 files"
   - Regex: `/(\d+)\s+(test|file|component|function)/i`
   - Default: 10 operations if no match

7. **Helper: classifyComplexity(tokens):**
   - < 20000: 'simple'
   - < 70000: 'moderate'
   - < 140000: 'complex'
   - >= 140000: 'very-complex'

Export: TaskChunker class, analyzeTask function (standalone), estimateTaskTokens function (standalone)
  </action>
  <verify>
```bash
node -e "const { TaskChunker, analyzeTask, estimateTaskTokens } = require('./get-shit-done/bin/task-chunker.js'); console.log('Module loads'); const c = new TaskChunker(); console.log('TaskChunker instantiates'); const r = analyzeTask({ description: 'Update 350 test files', files: [] }); console.log('analyzeTask works:', r.needsChunking, r.strategy)"
```
  </verify>
  <done>TaskChunker module created with analyzeTask, estimateTaskTokens, and createChunks methods. Module loads without errors.</done>
</task>

<task type="auto">
  <name>Task 2: Add task commands to gsd-tools.js</name>
  <files>get-shit-done/bin/gsd-tools.js</files>
  <action>
Add task chunking CLI commands to gsd-tools.js:

1. **Import TaskChunker** at top with other requires:
   `const { TaskChunker, analyzeTask, estimateTaskTokens } = require('./task-chunker.js');`

2. **Add cmdTask function** with subcommands:

   **task analyze --description "..." [--files "glob"]:**
   - Parse task from arguments (description from --description, files from glob expansion)
   - Call analyzeTask(task)
   - Output JSON with: needsChunking, estimatedTokens, chunkCount, strategy, complexity
   - Human-readable summary: "Task requires chunking: 10 chunks via batch strategy"

   **task chunk --description "..." [--files "glob"] [--strategy batch|file-batch|semantic]:**
   - Parse task, call analyzeTask to get chunks
   - Output JSON array of chunks with: id, description, range/files, estimatedTokens
   - If strategy override provided, use that instead of auto-detected

   **task batch --description "..." --count N:**
   - For repetitive operations, show batch breakdown
   - Extract operation count from description, divide by N batches
   - Output: batch ranges (e.g., "batch 1: items 1-35, batch 2: items 36-70")

   **task estimate --description "..." [--files count]:**
   - Quick estimate without full analysis
   - Call estimateTaskTokens with minimal task object
   - Output: { tokens, complexity, signals }

3. **Wire cmdTask** in main switch statement alongside existing commands (token, failure, completion)

4. **Help text** for task subcommands following existing patterns
  </action>
  <verify>
```bash
node get-shit-done/bin/gsd-tools.js task analyze --description "Update 350 test files to use new auth module"
node get-shit-done/bin/gsd-tools.js task estimate --description "Refactor user authentication" --files 15
node get-shit-done/bin/gsd-tools.js task batch --description "Update 350 tests" --count 10
```
  </verify>
  <done>gsd-tools.js handles task analyze/chunk/batch/estimate commands. Commands output JSON and work correctly.</done>
</task>

<task type="auto">
  <name>Task 3: Add batch processing coordinator support</name>
  <files>get-shit-done/bin/task-chunker.js</files>
  <action>
Extend TaskChunker with batch execution coordination features:

1. **BatchCoordinator class** (add to task-chunker.js):
   - Constructor: chunks array, progressCallback (optional)
   - Properties: completedChunks = [], failedChunks = [], currentChunk = null

2. **getNextChunk():**
   - Return next chunk not yet started
   - Mark as currentChunk
   - Return null if all complete

3. **markComplete(chunkId, result):**
   - Move from current to completedChunks
   - Store result (tokens used, files modified, etc.)
   - Call progressCallback if provided

4. **markFailed(chunkId, error):**
   - Move to failedChunks
   - Store error details
   - Return { retry: true, skip: false } for batch decisions

5. **getProgress():**
   - Return: { total, completed, failed, remaining, percentComplete }

6. **toJSON() / fromJSON(data):**
   - Serialize/deserialize for checkpoint resumption
   - Store: chunks, completedChunks, failedChunks

7. **CLI integration:**
   - Add `task progress` subcommand that reads .planning/batch_progress.json
   - Add `task resume` subcommand that resumes from checkpoint

Export: BatchCoordinator class alongside existing exports
  </action>
  <verify>
```bash
node -e "const { BatchCoordinator, TaskChunker } = require('./get-shit-done/bin/task-chunker.js'); const c = new TaskChunker(); const analysis = c.analyzeTask({ description: 'Update 100 tests' }); if (analysis.chunks) { const bc = new BatchCoordinator(analysis.chunks); console.log('Progress:', bc.getProgress()); console.log('Next:', bc.getNextChunk()?.id); }"
```
  </verify>
  <done>BatchCoordinator class created with getNextChunk/markComplete/markFailed/getProgress methods. Progress tracking and checkpoint serialization work.</done>
</task>

</tasks>

<verification>
1. `node get-shit-done/bin/task-chunker.js` - No import errors
2. `node get-shit-done/bin/gsd-tools.js task analyze --description "Update 350 test files"` - Returns JSON with needsChunking: true
3. `node get-shit-done/bin/gsd-tools.js task estimate --description "Add button to UI"` - Returns low token estimate (simple task)
4. `node get-shit-done/bin/gsd-tools.js task batch --description "Rename 50 functions" --count 5` - Returns 5 batch ranges
5. All three Phase 7 requirements covered: EXEC-19 (large task detection via analyzeTask), EXEC-20 (task chunking via createChunks), EXEC-22 (batch processing via BatchCoordinator)
</verification>

<success_criteria>
- TaskChunker.analyzeTask() detects large tasks (>140k estimated tokens)
- TaskChunker.createChunks() splits into manageable batches
- BatchCoordinator tracks progress through chunk execution
- CLI commands work: task analyze, task chunk, task batch, task estimate
- Repetitive operations detected and optimized (batch strategy)
</success_criteria>

<output>
After completion, create `.planning/phases/07-autonomous-execution-optimization/07-04-SUMMARY.md`
</output>
