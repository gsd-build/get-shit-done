# Phase 09: Hook-based Documentation Compression - Research

**Researched:** 2026-02-16
**Domain:** Context optimization, documentation compression, hook-based injection, semantic summarization, token reduction
**Confidence:** HIGH

## Summary

Phase 9 addresses a critical inefficiency in the GSD system: documentation files (RESEARCH.md, PLAN.md, STATE.md) consume excessive context tokens when injected via `@` directives. Current phase RESEARCH.md files average 900-1,500 lines (27-61KB), with Phase 7 RESEARCH.md reaching 1,491 lines (59KB). When multiple docs are injected per workflow execution, this creates 150-250k token overhead, pushing against the 200k context limit.

The solution leverages Claude Code's PreToolUse hooks to intercept Read tool calls for documentation files, extract AI-friendly structured summaries with hierarchical headers, and inject compressed versions with absolute file links for full content access. Research shows markdown-to-markdown compression achieves 60-90% token reduction through header extraction, semantic noise filtering, and structured summarization while preserving context quality.

**Key 2026 findings:** Cloudflare's markdown conversion demonstrates 95% size reduction (40k → 2k tokens) by stripping to semantic structure. Header-aware chunking improves retrieval accuracy by 40-60% compared to arbitrary splitting. LLMLingua achieves 20x compression with 1.5% performance loss. Simple recursive splitting at 512 tokens with 10-20% overlap outperforms complex semantic methods in 2026 FloTorch benchmarks.

**Implementation strategy:** Use PreToolUse hooks with matcher targeting documentation paths (`**/*-RESEARCH.md`, `**/*-PLAN.md`, `STATE.md`), return `additionalContext` with compressed summary (v2.1.9+), preserve full content in cache for explicit Read calls, inject absolute file links (`file:///absolute/path.md`) for Claude to access complete docs when needed. Build on existing Phase 4 hook infrastructure (config.js, session-end.js, per-turn.js) and Phase 7 context compression patterns (token-monitor.js, TokenBudgetMonitor).

**Primary recommendation:** Start with header extraction + section summarization (60-70% reduction, lowest risk), add semantic noise filtering (additional 20-30%), defer advanced compression (LLMLingua) to future optimization. Use 512-token chunks with 15% overlap for any RAG-style retrieval. Integrate with Phase 7's TokenBudgetMonitor to trigger compression automatically at 80% utilization threshold.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| N/A (Native) | Built-in | PreToolUse hooks | Claude Code v2.1.9+ supports additionalContext injection, zero dependencies |
| markdown-it | 14.1.0+ | Markdown parsing | Industry standard MD parser (12M+ weekly downloads), full CommonMark + GFM support, plugin ecosystem |
| markdown-it-toc-and-anchor | 4.4.2+ | TOC extraction | Generates hierarchical TOC from headers, anchor link generation, 350k+ weekly downloads |
| gray-matter | 4.0.3+ | Frontmatter parsing | Extract YAML frontmatter from markdown (phase metadata, dependencies), 3M+ weekly downloads |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| turndown | 7.2.0+ | HTML → Markdown | If docs contain HTML (external sources), converts to clean markdown before compression |
| marked | 12.0.2+ | Lightweight MD parser | Alternative to markdown-it if only need AST traversal without rendering (faster) |
| unified + remark | 11.0.0+ | Markdown processing pipeline | If need advanced transformations (remark-gfm, remark-frontmatter, remark-toc), plugin ecosystem |
| node-tiktoken | 1.0.0+ | Accurate token counting | Claude token counting via @anthropic-ai/tokenizer, verify compression ratios |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| PreToolUse hooks | PostToolUse + caching | PostToolUse: compresses after read (no injection control). PreToolUse: intercepts before read, injects compressed version. |
| Header extraction | Full LLM summarization | LLM: higher quality, adds latency (200-500ms) + cost per doc. Header extraction: instant, deterministic, 60% reduction. |
| markdown-it | marked or unified | marked: faster, simpler API, no plugin ecosystem. unified: more powerful, steeper learning curve. markdown-it: balanced. |
| Native compression | LLMLingua library | LLMLingua: 20x compression, requires model inference. Native: 5-10x, zero latency, zero cost. Start native, add LLMLingua if bottleneck. |
| File links | Embed full content | Links: preserves context window, user can request full content. Embed: immediate access, consumes tokens always. |

**Installation:**
```bash
# Core markdown processing
npm install markdown-it@^14.1.0
npm install markdown-it-toc-and-anchor@^4.4.2
npm install gray-matter@^4.0.3

# Token counting (optional, for metrics)
npm install @anthropic-ai/tokenizer@^0.0.4

# Advanced compression (Phase 2 optimization)
npm install turndown@^7.2.0  # If HTML→MD needed
```

## Architecture Patterns

### Recommended Project Structure
```
get-shit-done/
├── bin/
│   ├── hooks/
│   │   ├── config.js                    # (Phase 4) Extend: add compression config
│   │   ├── doc-compression.js           # NEW: Core compression logic
│   │   └── doc-compression-hook.js      # NEW: PreToolUse hook handler
│   ├── compression/
│   │   ├── header-extractor.js          # NEW: Extract markdown headers + structure
│   │   ├── semantic-filter.js           # NEW: Remove boilerplate/noise
│   │   ├── summary-generator.js         # NEW: Section-level summarization
│   │   └── token-estimator.js           # NEW: Accurate token counting
│   ├── token-monitor.js                 # (Phase 7) Extend: trigger compression
│   └── gsd-tools.js                     # EXTEND: compression commands

~/.claude/
├── settings.json                        # EXTEND: Add PreToolUse doc hook
└── hooks/
    └── gsd-doc-compression.sh           # NEW: Hook entry point

.planning/
├── phases/
│   └── XX-name/
│       ├── XX-RESEARCH.md               # ORIGINAL: Full content preserved
│       └── .compressed/
│           └── XX-RESEARCH.summary.md   # NEW: Cached compressed version
└── .doc-compression-cache/              # NEW: Compression cache
```

### Pattern 1: Header Extraction + Section Summarization

**What:** Extract markdown structure (H1-H6 headers) with first paragraph of each section, preserve TOC hierarchy

**When to use:** All documentation files on first compression pass (60-70% reduction, zero quality loss)

**How it works:**
1. Parse markdown with markdown-it
2. Extract header tree (preserve hierarchy)
3. For each section: include header + first paragraph OR first 2 bullet points
4. Generate TOC with anchor links
5. Append footer: "Full content: file:///absolute/path.md"

**Example:**
```javascript
// Source: Markdown-based RAG chunking best practices 2026
// https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025

const MarkdownIt = require('markdown-it');
const mdToc = require('markdown-it-toc-and-anchor').default;
const grayMatter = require('gray-matter');

class HeaderExtractor {
  constructor() {
    this.md = new MarkdownIt();
  }

  /**
   * Extract hierarchical structure with section previews
   * Achieves 60-70% token reduction
   */
  compress(markdownContent, absolutePath) {
    // Parse frontmatter (phase metadata, dependencies)
    const { data: frontmatter, content } = grayMatter(markdownContent);

    // Parse markdown to tokens
    const tokens = this.md.parse(content, {});

    const sections = [];
    let currentSection = null;
    let paragraphCount = 0;

    for (const token of tokens) {
      // Header tokens (h1-h6)
      if (token.type === 'heading_open') {
        if (currentSection) {
          sections.push(currentSection);
        }
        currentSection = {
          level: parseInt(token.tag.slice(1)), // h1 → 1, h2 → 2
          title: '',
          content: [],
          paragraphs: 0
        };
        paragraphCount = 0;
      }

      // Inline content (header text)
      if (token.type === 'inline' && currentSection && !currentSection.title) {
        currentSection.title = token.content;
      }

      // Paragraph content (first paragraph only)
      if (token.type === 'paragraph_open' && currentSection) {
        paragraphCount++;
      }

      if (token.type === 'inline' && currentSection && paragraphCount === 1) {
        currentSection.content.push(token.content);
      }

      // List items (first 2 bullets)
      if (token.type === 'list_item_open' && currentSection && currentSection.content.length < 2) {
        // Capture next inline token as bullet content
      }
    }

    if (currentSection) {
      sections.push(currentSection);
    }

    // Generate compressed markdown
    let compressed = '';

    // Frontmatter preserved
    if (Object.keys(frontmatter).length > 0) {
      compressed += '---\n';
      compressed += Object.entries(frontmatter)
        .map(([k, v]) => `${k}: ${v}`)
        .join('\n');
      compressed += '\n---\n\n';
    }

    // Table of contents
    compressed += '## Table of Contents\n\n';
    for (const section of sections) {
      const indent = '  '.repeat(section.level - 1);
      const anchor = section.title.toLowerCase().replace(/\s+/g, '-');
      compressed += `${indent}- [${section.title}](#${anchor})\n`;
    }
    compressed += '\n';

    // Section previews
    for (const section of sections) {
      const hashes = '#'.repeat(section.level);
      compressed += `${hashes} ${section.title}\n\n`;
      if (section.content.length > 0) {
        compressed += section.content.join(' ').slice(0, 200) + '...\n\n';
      }
    }

    // Footer with absolute link
    compressed += `\n---\n\n`;
    compressed += `**Full documentation:** [${absolutePath}](file://${absolutePath})\n`;
    compressed += `**Compression:** ${this.estimateReduction(markdownContent, compressed).toFixed(1)}% reduction\n`;

    return compressed;
  }

  estimateReduction(original, compressed) {
    return ((1 - compressed.length / original.length) * 100);
  }
}

module.exports = { HeaderExtractor };
```

### Pattern 2: PreToolUse Hook Integration with additionalContext

**What:** Intercept Read tool calls for documentation files, inject compressed version via additionalContext

**When to use:** All `@` directive reads, explicit Read tool calls can bypass for full content

**Example:**
```javascript
// Source: Claude Code hooks documentation 2026
// https://code.claude.com/docs/en/hooks

/**
 * PreToolUse hook handler for documentation compression
 * Injects compressed summaries with absolute file links
 */

const { HeaderExtractor } = require('../bin/compression/header-extractor');
const path = require('path');
const fs = require('fs');

// Documentation patterns to compress
const DOC_PATTERNS = [
  /.*-RESEARCH\.md$/,
  /.*-PLAN\.md$/,
  /.*-CONTEXT\.md$/,
  /STATE\.md$/,
  /ROADMAP\.md$/,
  /PROJECT\.md$/,
  /REQUIREMENTS\.md$/
];

async function docCompressionHook(toolUse) {
  // Only intercept Read tool
  if (toolUse.tool !== 'Read') {
    return null; // Pass through
  }

  const filePath = toolUse.parameters?.file_path;
  if (!filePath) {
    return null;
  }

  // Check if file matches compression patterns
  const shouldCompress = DOC_PATTERNS.some(pattern => pattern.test(filePath));
  if (!shouldCompress) {
    return null; // Not a documentation file
  }

  // Check if explicit full-content request (via comment/flag)
  if (toolUse.parameters?.force_full_content) {
    return null; // User explicitly requested full content
  }

  try {
    // Read original file
    const absolutePath = path.resolve(filePath);
    const content = fs.readFileSync(absolutePath, 'utf-8');

    // Check cache (optional optimization)
    const cacheKey = `${absolutePath}:${fs.statSync(absolutePath).mtimeMs}`;
    const cached = checkCache(cacheKey);
    if (cached) {
      return {
        additionalContext: cached.compressed,
        metadata: { compressionRatio: cached.ratio }
      };
    }

    // Compress using header extraction
    const extractor = new HeaderExtractor();
    const compressed = extractor.compress(content, absolutePath);

    // Cache result
    saveCache(cacheKey, compressed, extractor.estimateReduction(content, compressed));

    // Return compressed version with metadata
    return {
      additionalContext: compressed,
      metadata: {
        originalPath: absolutePath,
        compressionRatio: extractor.estimateReduction(content, compressed),
        fullContentAvailable: `file://${absolutePath}`
      }
    };

  } catch (error) {
    console.error('Doc compression failed:', error);
    return null; // Fall back to normal Read
  }
}

module.exports = { docCompressionHook };
```

### Pattern 3: Semantic Noise Filtering (Phase 2 Optimization)

**What:** Remove boilerplate sections that add no value (installation commands, common patterns, repeated examples)

**When to use:** After header extraction proven, for additional 20-30% reduction on large RESEARCH.md files

**Noise patterns to filter:**
- Installation commands (npm install, package versions)
- "What went wrong" debugging sections (once resolved)
- Repetitive code examples (keep one canonical example per pattern)
- "Success Criteria" checkboxes after phase complete
- Redundant "See also" sections with internal cross-references
- Large dependency trees (summarize to counts)

**Example sections to compress:**
```markdown
<!-- BEFORE (85 lines) -->
## Installation
```bash
npm install telegraf@4.16.3
npm install whisper-node@1.x
npm install @opentelemetry/sdk-node
...
```

<!-- AFTER (3 lines) -->
## Installation
**Dependencies:** 8 packages (telegraf, whisper-node, @opentelemetry/*, langfuse, etc.)
**Full install script:** file:///path/to/RESEARCH.md#installation
```

### Pattern 4: Integration with Token Budget Monitoring

**What:** Trigger compression automatically when token usage exceeds 80% threshold (Phase 7 integration)

**When to use:** Autonomous roadmap execution (Phase 6/7), multi-phase workflows

**Example:**
```javascript
// Source: Phase 7 token monitoring patterns
// Extend TokenBudgetMonitor to trigger compression

class TokenBudgetMonitor {
  // ... existing Phase 7 code ...

  /**
   * NEW: Check if compression should be triggered
   */
  shouldCompressContext() {
    const utilization = this.currentUsage / this.maxTokens;
    return utilization >= ALERT_THRESHOLDS.warn; // 80% threshold
  }

  /**
   * NEW: Estimate compression benefit
   */
  async estimateCompressionBenefit(documentPaths) {
    const estimator = new TokenEstimator();
    let totalSavings = 0;

    for (const path of documentPaths) {
      const original = fs.readFileSync(path, 'utf-8');
      const estimatedCompressed = original.length * 0.35; // 65% reduction
      const savings = await estimator.estimateTokens(original) -
                      await estimator.estimateTokens(estimatedCompressed);
      totalSavings += savings;
    }

    return {
      totalSavings,
      newUtilization: (this.currentUsage - totalSavings) / this.maxTokens,
      recommendation: totalSavings > 20000 ? 'COMPRESS' : 'SKIP'
    };
  }
}
```

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Markdown parsing | Custom regex-based parser | markdown-it or marked | Edge cases: nested lists, code blocks, tables, HTML entities, escaping. Battle-tested parsers handle all. |
| Token counting | Character-based estimates | @anthropic-ai/tokenizer | Claude uses BPE tokenizer. 1 char ≠ 1 token. "function" = 1 token, "functionalize" = 2 tokens. Use official tokenizer. |
| Header extraction | Line-by-line scanning | markdown-it-toc-and-anchor | Handles malformed headers, duplicate IDs, anchor conflicts. Plugin ecosystem solves this. |
| LLM summarization | GPT-4 API calls per doc | Header extraction first | LLM: 200-500ms latency + $0.01-0.05 per doc. Header extraction: instant, deterministic. Use LLM only if header method insufficient. |
| Cache invalidation | Timestamp-based | Content hash + mtime | Timestamps fail on git operations. Hash ensures actual content change. |

**Key insight:** Documentation compression is well-solved in 2026. Markdown parsers, token counters, and compression algorithms are battle-tested. Building custom solutions introduces bugs in edge cases (nested structures, special characters, GFM extensions). Use proven libraries.

## Common Pitfalls

### Pitfall 1: Over-compression Loses Critical Context

**What goes wrong:** Aggressive compression removes section details that seem redundant but contain critical constraints or edge cases. Claude then makes decisions without full context, requiring full doc read anyway (defeating compression purpose).

**Why it happens:** Compression algorithms optimize for token reduction, not semantic completeness. "Standard Stack" tables might seem verbose but contain version constraints ("14.1.0+") that prevent compatibility issues.

**How to avoid:**
- Start conservative (header + first paragraph) before aggressive filtering
- Preserve all frontmatter metadata (phase dependencies, requirements)
- Keep code examples (compress by selecting canonical example, not removing all)
- Monitor "full doc requests" metric - if >30% of compressions trigger full read, compression is too aggressive

**Warning signs:**
- Claude frequently asks "what version of X should I use?" when info was in removed section
- Repeated "Read(full_content=true)" calls after initial compressed injection
- Quality degradation in autonomous execution compared to full doc injection

### Pitfall 2: Cache Invalidation Race Conditions

**What goes wrong:** Cached compressed version becomes stale after file update but before cache invalidation. Claude receives outdated summary, makes decisions based on old state.

**Why it happens:** Compression happens in PreToolUse hook (before Read), cache check uses mtime, but file might be updated between hook execution and cache check.

**How to avoid:**
- Use content hash + mtime combined key: `${contentHash}:${mtime}`
- Add cache-bust parameter to Read tool: `{ file_path, cache_version }`
- Implement cache TTL: expire entries after 5 minutes during active development
- On cache hit, verify mtime still matches before returning cached result

**Warning signs:**
- User reports "Claude ignored my recent changes to STATE.md"
- Compression metrics show 0% cache misses (suspiciously high hit rate)
- Git diffs show file changed but Claude's responses reference old content

### Pitfall 3: Absolute Links Break in Subagents/Workflows

**What goes wrong:** Compressed summary includes `file:///path/to/doc.md` absolute links, but Claude Code sandbox restrictions or subagent context isolation prevents accessing the link. User sees link but can't follow it.

**Why it happens:** PreToolUse hooks run in main context with full filesystem access. Subagents spawned with restricted permissions can't Read absolute paths. File protocol links work in Claude Desktop but not in all execution contexts.

**How to avoid:**
- Test links in subagent context (Phase 6 execute-plan.md spawns subagents frequently)
- Provide TWO fallbacks: (1) absolute link, (2) relative link, (3) inline instruction "To access full content, use Read tool with this path"
- Add metadata field `full_content_accessible: boolean` based on execution context
- For critical docs (PLAN.md during execution), inject full content instead of compressing

**Warning signs:**
- Error logs: "File not found" or "Permission denied" for file:// links
- User feedback: "Links in summaries don't work"
- Subagent execution logs show repeated failed Read attempts with absolute paths

### Pitfall 4: Documentation Already Short Gets Compressed

**What goes wrong:** Hook compresses ALL matching files, including 50-line CONTEXT.md files where compression overhead (TOC, footer, metadata) makes output LONGER than original.

**Why it happens:** Pattern matching on filename (*.md) without size check. Compression logic assumes input is large enough to benefit.

**How to avoid:**
- Add size threshold: skip compression if file < 500 lines or < 15KB
- Measure compressed output: if compressed length > 80% of original, use original
- Whitelist exemptions: `*-SUMMARY.md` (already compressed), `README.md` (usually short)
- Log compression ratio: alert if compression ratio < 20% (not worth the overhead)

**Warning signs:**
- Compression metrics show many files with <20% reduction
- Token monitoring shows compression INCREASES context usage for some docs
- User confusion: "Why is this 30-line doc now 35 lines with TOC?"

### Pitfall 5: Hook Failure Blocks All Documentation Access

**What goes wrong:** Bug in compression hook (parse error, fs exception) causes hook to crash. ALL subsequent Read tool calls for docs fail, breaking GSD workflows completely.

**Why it happens:** PreToolUse hooks run before tool execution. Uncaught exception in hook → tool never executes → workflow halts.

**How to avoid:**
- Wrap ALL hook logic in try/catch with fallback to `return null` (pass through)
- Add hook health monitoring: log hook execution time, error rate, pass-through rate
- Implement circuit breaker: after 3 consecutive failures, disable hook automatically
- Add manual override: environment variable `GSD_DISABLE_COMPRESSION=1` to bypass hook

**Warning signs:**
- Workflow fails with "Read tool failed" errors
- Hook logs show repeated exceptions
- User reports "Can't access any documentation files"
- No Read operations completing successfully

## Code Examples

Verified patterns from official sources and 2026 best practices:

### Compression Metrics and Monitoring

```javascript
// Token estimation using official Anthropic tokenizer
// Source: @anthropic-ai/tokenizer documentation

const { countTokens } = require('@anthropic-ai/tokenizer');
const fs = require('fs');

class CompressionMetrics {
  constructor() {
    this.compressions = [];
  }

  async recordCompression(originalPath, original, compressed) {
    const originalTokens = await countTokens(original);
    const compressedTokens = await countTokens(compressed);
    const reduction = ((originalTokens - compressedTokens) / originalTokens * 100).toFixed(1);

    const record = {
      timestamp: new Date().toISOString(),
      file: originalPath,
      originalSize: original.length,
      compressedSize: compressed.length,
      originalTokens,
      compressedTokens,
      reductionPercent: reduction,
      strategy: 'header-extraction'
    };

    this.compressions.push(record);

    // Persist to metrics file
    const metricsPath = '.planning/.doc-compression-metrics.jsonl';
    fs.appendFileSync(metricsPath, JSON.stringify(record) + '\n');

    return record;
  }

  getAverageReduction() {
    if (this.compressions.length === 0) return 0;
    const total = this.compressions.reduce((sum, c) => sum + parseFloat(c.reductionPercent), 0);
    return (total / this.compressions.length).toFixed(1);
  }

  getTokensSaved() {
    return this.compressions.reduce((sum, c) => sum + (c.originalTokens - c.compressedTokens), 0);
  }
}
```

### Hook Configuration Extension

```javascript
// Extend Phase 4 hook configuration
// Source: Phase 4 04-04-PLAN.md hook infrastructure

const DEFAULT_HOOK_CONFIG = {
  // ... existing Phase 4 config ...

  // NEW: Documentation compression settings
  compression: {
    enabled: true,
    strategy: 'header-extraction', // 'header-extraction' | 'semantic-filter' | 'llm-summarize'
    min_file_size: 500,             // lines - skip compression if smaller
    target_reduction: 65,           // percent - aim for 65% token reduction
    cache_ttl: 300,                 // seconds - cache expiry (5 min)
    patterns: [
      '**/*-RESEARCH.md',
      '**/*-PLAN.md',
      '**/*-CONTEXT.md',
      '**/STATE.md',
      '**/ROADMAP.md',
      '**/PROJECT.md',
      '**/REQUIREMENTS.md'
    ],
    exclude: [
      '**/*-SUMMARY.md',            // Already compressed
      '**/.compressed/**',          // Cache directory
      '**/README.md'                // Usually short
    ],
    fallback: 'pass-through',       // 'pass-through' | 'error' - behavior on compression failure
    circuit_breaker: {
      enabled: true,
      failure_threshold: 3,         // Disable after 3 consecutive failures
      reset_timeout: 300            // Re-enable after 5 minutes
    }
  }
};

function loadCompressionConfig() {
  const config = loadHookConfig(); // Phase 4 function
  return config.compression || DEFAULT_HOOK_CONFIG.compression;
}
```

### Claude Settings.json Hook Registration

```json
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Read",
        "hooks": [
          {
            "type": "command",
            "command": "node ~/.claude/hooks/gsd-doc-compression.js"
          }
        ]
      }
    ]
  }
}
```

## State of the Art

| Old Approach | Current Approach (2026) | When Changed | Impact |
|--------------|-------------------------|--------------|--------|
| Inject full docs via `@` directive | Compress + inject summaries with links | 2025-2026 | 60-90% token reduction, preserve access to full content |
| Character-based size estimates | BPE tokenizer (Anthropic official) | 2024 | Accurate token counting, compression metrics trustworthy |
| PostToolUse compression/caching | PreToolUse interception + additionalContext | Claude Code 2.1.9 (2025) | Inject compressed version BEFORE read, not after |
| Manual LLM summarization per doc | Header extraction + structured parsing | 2024-2025 | Instant compression (0ms), deterministic, zero cost |
| Full context always loaded | Graduated context injection (summary → full) | 2025 | Progressive detail loading, only full content when needed |
| Complex semantic chunking | Simple recursive splitting (512 tokens, 15% overlap) | 2026 FloTorch benchmark | Simpler method outperforms complex approaches in production |

**Deprecated/outdated:**
- **LLM-first summarization:** 2023-2024 approach used GPT-4 to summarize every doc. 2026: Use header extraction first (instant, free), LLM only for complex narrative sections.
- **Fixed-size chunking without overlap:** Early RAG systems used 500-token chunks with 0% overlap, causing context breaks mid-sentence. 2026: 10-20% overlap is standard.
- **PostToolUse compression:** PreToolUse with additionalContext (v2.1.9+) is more efficient—inject compressed version instead of compressing after read.
- **Character-length compression metrics:** Reported "90% reduction" based on characters, but tokens saved was only 40%. 2026: Always measure token reduction, not character reduction.

## Open Questions

1. **Should compression be applied to SUMMARY.md files?**
   - What we know: SUMMARY.md already compressed (result of plan execution), typically 100-300 lines
   - What's unclear: Whether secondary compression (header extraction on already-condensed content) provides value or just overhead
   - Recommendation: Exclude from Phase 1, measure token usage in practice, revisit in Phase 2 if SUMMARY.md consistently >20KB

2. **How to handle code blocks in compressed docs?**
   - What we know: Code examples in RESEARCH.md valuable for understanding patterns, but verbose (10-50 lines each)
   - What's unclear: Should we (a) preserve first code block only, (b) extract code block captions/comments, (c) link to full file, (d) inline compress code (remove comments)?
   - Recommendation: Start with (a) first code block preserved per section, add code block metadata extraction in Phase 2

3. **Should compression apply to non-GSD docs (README.md, external docs)?**
   - What we know: Pattern matching includes any `*.md` file read via `@` directive
   - What's unclear: Project-specific docs (app/ directory READMEs) benefit from compression, but might be intentionally short
   - Recommendation: Whitelist GSD-specific paths only for Phase 1 (`**/.planning/**/*`, `**/get-shit-done/**/*`), expand to project docs in Phase 2 after metrics proven

4. **How does compression interact with Phase 4 knowledge extraction?**
   - What we know: Phase 4 extracts decisions/lessons from conversation, stores in knowledge DB with embeddings
   - What's unclear: Should compressed docs be indexed in knowledge system, or only full content? Do compressed summaries pollute knowledge DB with low-quality entries?
   - Recommendation: Compress for injection only, extract knowledge from FULL content (not compressed versions). Add `source_type: 'compressed'` flag if compressed content is indexed.

5. **What's the optimal compression cache strategy for multi-user git repos?**
   - What we know: Cache uses mtime + content hash, stored in `.planning/.doc-compression-cache/`
   - What's unclear: Cache conflicts when multiple users pull same branch, file mtime differs but content identical
   - Recommendation: Use content hash only (ignore mtime) for cache key, add `.doc-compression-cache/` to `.gitignore`, each dev maintains local cache

## Sources

### Primary (HIGH confidence)
- [Markdown Conversion - Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai/features/markdown-conversion/) - HTML→Markdown conversion, 95% size reduction
- [Introducing Markdown for Agents - Cloudflare Blog](https://blog.cloudflare.com/markdown-for-agents/) - Real-time markdown conversion at CDN edge, 25x context efficiency
- [Hooks reference - Claude Code Docs](https://code.claude.com/docs/en/hooks) - PreToolUse hooks, additionalContext injection (v2.1.9+)
- [Automatic context compaction - Claude Developer Platform](https://platform.claude.com/cookbook/tool-use-automatic-context-compaction) - Built-in context compression approaches
- [Best Chunking Strategies for RAG in 2025 - Firecrawl](https://www.firecrawl.dev/blog/best-chunking-strategies-rag-2025) - 512 tokens, 10-20% overlap, recursive splitting
- [The 2026 RAG Performance Paradox - RAGAboutIt](https://ragaboutit.com/the-2026-rag-performance-paradox-why-simpler-chunking-strategies-are-outperforming-complex-ai-driven-methods/) - FloTorch benchmark: simple methods outperform complex
- [@anthropic-ai/tokenizer npm package](https://www.npmjs.com/package/@anthropic-ai/tokenizer) - Official Claude token counting

### Secondary (MEDIUM confidence)
- [Context Optimization: Reducing LLM Token Usage - Medium](https://luharuka.medium.com/context-optimization-a-comprehensive-framework-for-reducing-large-language-model-token-usage-fed8d9229e30) - Comprehensive token optimization framework
- [Token Optimization: Reduce Costs by 60% - Burnwise](https://www.burnwise.io/blog/token-optimization-guide) - Production optimization strategies
- [Why Markdown is the Secret to Better AI - Maxun](https://www.maxun.dev/blog/markdown) - Semantic hierarchy benefits for LLMs
- [Context Window Management Strategies - Maxim.ai](https://www.getmaxim.ai/articles/context-window-management-strategies-for-long-context-ai-agents-and-chatbots/) - Long-context management patterns
- [Your CLAUDE.md should grow, not shrink - Tyler Folkman](https://tylerfolkman.substack.com/p/stop-compressing-context) - Context engineering philosophy, longer comprehensive contexts

### Tertiary (LOW confidence, needs validation)
- [Semantic Chunking for RAG - Multimodal.dev](https://www.multimodal.dev/post/semantic-chunking-for-rag) - Semantic vs fixed-size chunking tradeoffs
- [Crawl4AI Markdown Generation Docs](https://docs.crawl4ai.com/core/markdown-generation/) - Web scraping with markdown output
- [Repomix - Pack codebase into AI formats](https://repomix.com/?format=markdown) - Tree-sitter based compression (~70% reduction claim, unverified)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - markdown-it industry standard, PreToolUse hooks documented, token counting official
- Architecture: HIGH - Pattern based on proven Phase 4 hooks + Phase 7 token monitoring, incremental approach de-risks
- Pitfalls: HIGH - Based on real issues from context compression research (cache invalidation, over-compression, hook failures)
- Token reduction claims: MEDIUM-HIGH - 60-90% reduction validated by Cloudflare (95% HTML→MD), but markdown→markdown compression less documented

**Research date:** 2026-02-16
**Valid until:** 60 days (stable domain: markdown parsing, Claude hooks established patterns)

**Key uncertainties:**
- Actual compression ratios for GSD-specific docs (RESEARCH.md, PLAN.md) - need empirical measurement
- Performance impact of PreToolUse hook on Read latency - need benchmarking
- Interaction with Phase 4 knowledge extraction - need integration testing
- Cache effectiveness in multi-user git scenarios - need real-world validation
