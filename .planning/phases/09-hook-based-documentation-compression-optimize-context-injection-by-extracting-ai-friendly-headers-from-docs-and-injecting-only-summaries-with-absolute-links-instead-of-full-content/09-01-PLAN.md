---
phase: 09-hook-based-documentation-compression
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/compression/header-extractor.js
  - get-shit-done/bin/compression/token-estimator.js
  - get-shit-done/package.json
autonomous: true

must_haves:
  truths:
    - "Markdown documents can be parsed and compressed to 60-70% token reduction"
    - "Header hierarchy is preserved in compressed output"
    - "Token counts are accurate using official Anthropic tokenizer"
    - "Compressed output includes absolute file link for full content access"
  artifacts:
    - path: "get-shit-done/bin/compression/header-extractor.js"
      provides: "HeaderExtractor class with compress() method"
      exports: ["HeaderExtractor"]
    - path: "get-shit-done/bin/compression/token-estimator.js"
      provides: "Accurate token counting using @anthropic-ai/tokenizer"
      exports: ["TokenEstimator", "estimateTokens"]
  key_links:
    - from: "get-shit-done/bin/compression/header-extractor.js"
      to: "markdown-it"
      via: "MD parsing"
      pattern: "require.*markdown-it"
    - from: "get-shit-done/bin/compression/token-estimator.js"
      to: "@anthropic-ai/tokenizer"
      via: "countTokens import"
      pattern: "require.*@anthropic-ai/tokenizer"
---

<objective>
Create the core header extraction and token estimation modules for documentation compression.

Purpose: Foundation for hook-based compression - these modules parse markdown, extract hierarchical headers with first-paragraph previews, and provide accurate token counting for compression metrics.

Output: Two Node.js modules in `get-shit-done/bin/compression/` directory with npm dependencies installed.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-hook-based-documentation-compression-optimize-context-injection-by-extracting-ai-friendly-headers-from-docs-and-injecting-only-summaries-with-absolute-links-instead-of-full-content/09-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install compression dependencies</name>
  <files>get-shit-done/package.json</files>
  <action>
    Install required npm packages for markdown processing and token counting:

    ```bash
    cd get-shit-done && npm install markdown-it@^14.1.0 gray-matter@^4.0.3 @anthropic-ai/tokenizer@^0.0.4
    ```

    These provide:
    - markdown-it: Industry-standard markdown parser with token stream access
    - gray-matter: YAML frontmatter extraction (phase metadata, dependencies)
    - @anthropic-ai/tokenizer: Official Claude BPE tokenizer for accurate counting

    Do NOT install markdown-it-toc-and-anchor - we'll extract TOC manually for more control.
  </action>
  <verify>
    ```bash
    cd get-shit-done && npm ls markdown-it gray-matter @anthropic-ai/tokenizer
    ```
    All three packages should appear in dependency tree without errors.
  </verify>
  <done>Package.json updated with compression dependencies, node_modules populated</done>
</task>

<task type="auto">
  <name>Task 2: Create HeaderExtractor module</name>
  <files>get-shit-done/bin/compression/header-extractor.js</files>
  <action>
    Create `get-shit-done/bin/compression/header-extractor.js` with HeaderExtractor class:

    **Class: HeaderExtractor**
    - Constructor: Initialize markdown-it parser
    - Method: `compress(markdownContent, absolutePath)` returns compressed markdown string
    - Method: `estimateReduction(original, compressed)` returns percentage reduction

    **Compression algorithm:**
    1. Parse frontmatter using gray-matter (preserve all YAML metadata)
    2. Parse markdown body to token stream using markdown-it
    3. Extract sections: header_open -> inline (title) -> paragraph_open -> inline (first paragraph)
    4. For each section: keep header + first paragraph only (truncate to 200 chars)
    5. Generate TOC with anchor links from collected headers
    6. Append footer with absolute file link: `**Full documentation:** file://${absolutePath}`
    7. Include compression ratio in footer

    **Edge cases to handle:**
    - Empty sections (header with no paragraph): include header, skip content line
    - Nested lists: extract first 2 bullet points if no paragraph follows header
    - Code blocks after headers: skip code blocks in first pass (they're verbose)
    - Headers with special chars: slugify for anchor links (lowercase, replace spaces with -)

    **Output format:**
    ```markdown
    ---
    (preserved frontmatter)
    ---

    ## Table of Contents

    - [Section 1](#section-1)
      - [Subsection 1.1](#subsection-1-1)

    # Section 1

    First paragraph preview up to 200 chars...

    ## Subsection 1.1

    First paragraph preview...

    ---

    **Full documentation:** [/absolute/path/to/doc.md](file:///absolute/path/to/doc.md)
    **Compression:** 65.2% reduction
    ```

    Export: `{ HeaderExtractor }`
  </action>
  <verify>
    Create test file and run:
    ```bash
    cd get-shit-done && node -e "
    const { HeaderExtractor } = require('./bin/compression/header-extractor');
    const extractor = new HeaderExtractor();
    const sample = '# Title\n\nFirst paragraph with lots of content here.\n\n## Section\n\nAnother paragraph.\n';
    const result = extractor.compress(sample, '/test/doc.md');
    console.log(result);
    console.log('---');
    console.log('Reduction:', extractor.estimateReduction(sample, result) + '%');
    "
    ```
    Should output compressed markdown with TOC, truncated paragraphs, and file link footer.
  </verify>
  <done>HeaderExtractor class compresses markdown with 60-70% character reduction, preserves structure, includes TOC and absolute file link</done>
</task>

<task type="auto">
  <name>Task 3: Create TokenEstimator module</name>
  <files>get-shit-done/bin/compression/token-estimator.js</files>
  <action>
    Create `get-shit-done/bin/compression/token-estimator.js` with token counting utilities:

    **Class: TokenEstimator**
    - Method: `estimateTokens(text)` returns token count using Anthropic tokenizer
    - Method: `compareTokens(original, compressed)` returns `{ original, compressed, saved, reductionPercent }`

    **Function: estimateTokens(text)**
    - Standalone function for quick token estimation
    - Uses @anthropic-ai/tokenizer's countTokens function

    **Implementation notes:**
    - Import: `const { countTokens } = require('@anthropic-ai/tokenizer');`
    - Handle empty/null input gracefully (return 0)
    - Cache tokenizer instance if library supports it (check docs)

    **Export:** `{ TokenEstimator, estimateTokens }`

    **Metrics output format for compareTokens:**
    ```json
    {
      "original": 15234,
      "compressed": 5420,
      "saved": 9814,
      "reductionPercent": 64.4
    }
    ```
  </action>
  <verify>
    ```bash
    cd get-shit-done && node -e "
    const { TokenEstimator, estimateTokens } = require('./bin/compression/token-estimator');
    const est = new TokenEstimator();
    const original = 'This is a longer document with multiple paragraphs...'.repeat(100);
    const compressed = 'Summary: document overview...';
    console.log('Original tokens:', estimateTokens(original));
    console.log('Compressed tokens:', estimateTokens(compressed));
    console.log('Comparison:', est.compareTokens(original, compressed));
    "
    ```
    Should show accurate token counts and reduction percentage.
  </verify>
  <done>TokenEstimator provides accurate token counts using official Anthropic tokenizer, with comparison utilities for metrics</done>
</task>

</tasks>

<verification>
Run integration test combining both modules:

```bash
cd get-shit-done && node -e "
const { HeaderExtractor } = require('./bin/compression/header-extractor');
const { TokenEstimator } = require('./bin/compression/token-estimator');
const fs = require('fs');
const path = require('path');

// Test on actual RESEARCH.md file
const researchPath = path.resolve('../.planning/phases/09-hook-based-documentation-compression-optimize-context-injection-by-extracting-ai-friendly-headers-from-docs-and-injecting-only-summaries-with-absolute-links-instead-of-full-content/09-RESEARCH.md');
const original = fs.readFileSync(researchPath, 'utf-8');

const extractor = new HeaderExtractor();
const estimator = new TokenEstimator();

const compressed = extractor.compress(original, researchPath);
const metrics = estimator.compareTokens(original, compressed);

console.log('=== Compression Results ===');
console.log('Original lines:', original.split('\n').length);
console.log('Compressed lines:', compressed.split('\n').length);
console.log('Token reduction:', metrics.reductionPercent.toFixed(1) + '%');
console.log('Tokens saved:', metrics.saved);
console.log('');
console.log('=== Compressed Preview (first 500 chars) ===');
console.log(compressed.slice(0, 500));
"
```

Expected: 50-70% token reduction on actual 09-RESEARCH.md file.
</verification>

<success_criteria>
- HeaderExtractor compresses markdown to 60-70% token reduction
- Frontmatter preserved intact
- TOC generated with working anchor links
- Absolute file link in footer
- TokenEstimator counts tokens accurately using official Anthropic tokenizer
- Both modules export cleanly and can be required without errors
</success_criteria>

<output>
After completion, create `.planning/phases/09-hook-based-documentation-compression-optimize-context-injection-by-extracting-ai-friendly-headers-from-docs-and-injecting-only-summaries-with-absolute-links-instead-of-full-content/09-01-SUMMARY.md`
</output>
