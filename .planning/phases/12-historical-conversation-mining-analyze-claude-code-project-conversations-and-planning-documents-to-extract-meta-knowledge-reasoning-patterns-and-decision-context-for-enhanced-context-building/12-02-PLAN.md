---
phase: 12-historical-conversation-mining
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - get-shit-done/bin/gsd-tools.js
autonomous: true

must_haves:
  truths:
    - "Running 'node gsd-tools.js mine-conversations' discovers, converts, quality-checks, and returns extraction requests for Claude Code conversations"
    - "Already-analyzed conversations are skipped via content hash check using conversation-specific analysis log"
    - "The --max-age-days flag controls how far back to scan (default 30)"
    - "The --include-subagents flag enables subagent file scanning"
    - "The --limit flag caps the number of files processed"
    - "The store-analysis-result command works for conversation mining results (marks conversation as analyzed in conversation analysis log)"
  artifacts:
    - path: "get-shit-done/bin/gsd-tools.js"
      provides: "mine-conversations CLI command and store-conversation-result command"
      contains: "cmdMineConversations"
  key_links:
    - from: "get-shit-done/bin/gsd-tools.js"
      to: "get-shit-done/bin/conversation-miner.js"
      via: "require('./conversation-miner.js') in cmdMineConversations"
      pattern: "require.*conversation-miner"
    - from: "get-shit-done/bin/gsd-tools.js"
      to: "get-shit-done/bin/session-quality-gates.js"
      via: "markSessionAnalyzed() for conversation analysis tracking"
      pattern: "markSessionAnalyzed"
---

<objective>
Add the `mine-conversations` CLI command to gsd-tools.js and a companion `store-conversation-result` command that marks conversations as analyzed using a conversation-specific analysis log.

Purpose: The CLI command is the interface between the mine-conversations.md workflow and the conversation-miner.js module. The workflow calls this command to discover and prepare conversations, then calls store-conversation-result to persist results. Using a separate analysis log path (.planning/knowledge/.conversation-analysis-log.jsonl) prevents mixing conversation and Telegram session tracking.

Output: Two new commands in gsd-tools.js: mine-conversations and store-conversation-result.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-RESEARCH.md
@.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-01-SUMMARY.md
@get-shit-done/bin/gsd-tools.js
@get-shit-done/bin/conversation-miner.js
@get-shit-done/bin/session-quality-gates.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add mine-conversations and store-conversation-result commands to gsd-tools.js</name>
  <files>get-shit-done/bin/gsd-tools.js</files>
  <action>
Add two new commands to gsd-tools.js. Follow the exact patterns used by cmdAnalyzeSession, cmdListPendingSessions, and cmdStoreAnalysisResult.

**1. Add `cmdMineConversations(cwd, args, raw)` function** (place near other session analysis commands around line 7477):

```
async function cmdMineConversations(cwd, args, raw) {
```

Parse options using the project's existing indexOf pattern:
- `--max-age-days N`: `args.includes('--max-age-days') ? parseInt(args[args.indexOf('--max-age-days') + 1]) : 30`
- `--include-subagents`: `args.includes('--include-subagents')`
- `--limit N`: `args.includes('--limit') ? parseInt(args[args.indexOf('--limit') + 1]) : 0`

Logic:
1. Lazy-require conversation-miner.js: `require(path.join(__dirname, 'conversation-miner.js'))`. On error, output `{ status: 'error', reason: '...' }` and return.
2. Call `conversationMiner.discoverProjectConversations(cwd, { maxAgeDays, includeSubagents })`.
3. If `error` in result, output `{ status: 'error', reason: error, projectSlugDir }` and return.
4. Slice files to limit if limit > 0: `const targetFiles = limit > 0 ? files.slice(0, limit) : files`.
5. For each file in targetFiles, call `conversationMiner.prepareConversationForMining(fileInfo.path, fileInfo.sessionId)`.
6. If `!prepared.shouldMine`, add to skipped array `{ sessionId, reason: prepared.reason }`.
7. If `prepared.shouldMine`, add to extractionSessions array: `{ sessionId, sessionPath: fileInfo.path, mtime: fileInfo.mtime, chunkCount: prepared.chunkCount, extractionRequests: prepared.extractionRequests }`.
8. Output JSON:
```json
{
  "status": "ready",
  "filesFound": files.length,
  "filesTargeted": targetFiles.length,
  "sessionsReady": extractionSessions.length,
  "sessionsSkipped": skipped.length,
  "sessions": extractionSessions,
  "skipped": skipped
}
```

**2. Add `cmdStoreConversationResult(cwd, args, raw)` function:**

This is nearly identical to cmdStoreAnalysisResult but uses a SEPARATE analysis log for conversations. The key difference: instead of calling `markSessionAnalyzed()` which writes to `.planning/telegram-sessions/.analysis-log.jsonl`, this command writes to `.planning/knowledge/.conversation-analysis-log.jsonl`.

Logic:
1. Parse args: `sessionId = args[0]`, `resultsArg = args.slice(1).join(' ')`
2. Validate sessionId and resultsArg exist
3. Parse results JSON (try JSON.parse first, then try as file path -- same pattern as cmdStoreAnalysisResult)
4. Lazy-require session-analyzer.js for parseExtractionResult, knowledge-writer.js for storeInsights
5. For each result object `{ type, result }`, call parseExtractionResult(type, result) to get insights
6. Call storeInsights(allInsights, { sessionId, conversationId: sessionId, scope: 'project' }) -- note: pass conversationId for context tracking
7. Mark as analyzed: Write a JSONL entry to `.planning/knowledge/.conversation-analysis-log.jsonl` containing `{ session_id: sessionId, content_hash: contentHash, analyzed_at: new Date().toISOString(), insight_count: totalInsights, version: 1 }`. The content hash should be passed as a `--content-hash` arg from the workflow, OR computed from the results if not provided. Use `args.includes('--content-hash') ? args[args.indexOf('--content-hash') + 1] : 'unknown'`.
8. Ensure parent directory exists before writing to the log file.
9. Output: `{ stored, skipped, evolved, errors }`.

**3. Update the CLI router** (the `switch(command)` block near line 8190):

Add two new cases before the `default:` case:
```javascript
case 'mine-conversations': {
  await cmdMineConversations(cwd, args.slice(1), raw);
  break;
}

case 'store-conversation-result': {
  await cmdStoreConversationResult(cwd, args.slice(1), raw);
  break;
}
```

**4. Update the help comment** at the top of gsd-tools.js (around line 128-133, in the Session Analysis section):

Add after `store-analysis-result`:
```
 *   mine-conversations                         Discover and prepare conversation mining
 *     [--max-age-days N] [--include-subagents] [--limit N]
 *   store-conversation-result <id> <json>      Store conversation mining results
 *     [--content-hash <hash>]
```
  </action>
  <verify>
1. Test mine-conversations command:
```bash
node get-shit-done/bin/gsd-tools.js mine-conversations --limit 3
```
Should return JSON with status, filesFound, sessionsReady, skipped arrays.

2. Test with max-age-days:
```bash
node get-shit-done/bin/gsd-tools.js mine-conversations --max-age-days 7 --limit 2
```
Should return fewer or equal files compared to 30-day default.

3. Test store-conversation-result with empty results:
```bash
node get-shit-done/bin/gsd-tools.js store-conversation-result test-session-123 '[]'
```
Should return `{ stored: 0, skipped: 0, evolved: 0, errors: [] }`.

4. Verify conversation analysis log separation:
```bash
ls -la .planning/knowledge/.conversation-analysis-log.jsonl 2>/dev/null
ls -la .planning/telegram-sessions/.analysis-log.jsonl 2>/dev/null
```
These should be separate files (or the conversation one may not exist yet until first real analysis).
  </verify>
  <done>
gsd-tools.js has mine-conversations and store-conversation-result commands. mine-conversations discovers JSONL files, converts entries, applies quality gates, checks re-analysis prevention against the conversation-specific analysis log, and returns extraction requests. store-conversation-result persists Haiku results and marks conversations as analyzed in the conversation analysis log.
  </done>
</task>

</tasks>

<verification>
- `node gsd-tools.js mine-conversations` runs without errors and returns JSON with sessions array
- `node gsd-tools.js mine-conversations --limit 1` returns at most 1 session
- `node gsd-tools.js mine-conversations --max-age-days 1` filters by age
- `node gsd-tools.js store-conversation-result test '[]'` returns success with 0 stored
- Conversation analysis log is separate from Telegram session analysis log
- conversation-miner.js's re-analysis check uses the conversation-specific log path
</verification>

<success_criteria>
Both CLI commands work end-to-end. mine-conversations returns extraction requests ready for Haiku Task() invocation. store-conversation-result persists results and prevents re-analysis. The conversation analysis log lives at .planning/knowledge/.conversation-analysis-log.jsonl, completely separate from Telegram session tracking.
</success_criteria>

<output>
After completion, create `.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-02-SUMMARY.md`
</output>
