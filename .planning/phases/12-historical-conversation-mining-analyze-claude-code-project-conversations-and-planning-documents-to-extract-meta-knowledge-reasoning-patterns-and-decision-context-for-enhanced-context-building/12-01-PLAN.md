---
phase: 12-historical-conversation-mining
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - get-shit-done/bin/conversation-miner.js
autonomous: true

must_haves:
  truths:
    - "Claude Code JSONL files are discovered by mapping project CWD to slug directory"
    - "Progress/system/file-history-snapshot entries are filtered out (90%+ noise removed)"
    - "Tool result user entries are skipped (not treated as human input)"
    - "XML command injections from GSD workflows are stripped from user messages"
    - "Conversations with fewer than 2 bot responses or under 500 chars total are rejected by quality gate"
    - "Converted entries use user_message/bot_response types compatible with formatEntriesForPrompt()"
  artifacts:
    - path: "get-shit-done/bin/conversation-miner.js"
      provides: "Format adapter converting Claude Code JSONL to session-like entries"
      exports: ["discoverProjectConversations", "convertConversationEntries", "shouldMineConversation", "prepareConversationForMining"]
      min_lines: 150
  key_links:
    - from: "get-shit-done/bin/conversation-miner.js"
      to: "get-shit-done/bin/session-chunker.js"
      via: "require('./session-chunker.js') for chunkSession()"
      pattern: "require.*session-chunker"
    - from: "get-shit-done/bin/conversation-miner.js"
      to: "get-shit-done/bin/session-analyzer.js"
      via: "require('./session-analyzer.js') for analyzeSession()"
      pattern: "require.*session-analyzer"
    - from: "get-shit-done/bin/conversation-miner.js"
      to: "get-shit-done/bin/session-quality-gates.js"
      via: "require('./session-quality-gates.js') for getSessionContentHash()"
      pattern: "require.*session-quality-gates"
---

<objective>
Create the conversation-miner.js module -- the format adapter that converts Claude Code project JSONL entries into session-like entries compatible with the Phase 11 extraction pipeline.

Purpose: This is the core deliverable of Phase 12. Claude Code stores conversations at ~/.claude/projects/{slug}/*.jsonl using user/assistant/progress/system entry types. The existing Phase 11 infrastructure (session-analyzer.js, analysis-prompts.js, session-chunker.js, knowledge-writer.js) expects user_message/bot_response types. conversation-miner.js bridges this gap.

Output: A single CommonJS module exporting four functions: discoverProjectConversations, convertConversationEntries, shouldMineConversation, prepareConversationForMining.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-RESEARCH.md
@get-shit-done/bin/session-analyzer.js
@get-shit-done/bin/session-quality-gates.js
@get-shit-done/bin/session-chunker.js
@get-shit-done/bin/analysis-prompts.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create conversation-miner.js with discovery, conversion, and quality gate functions</name>
  <files>get-shit-done/bin/conversation-miner.js</files>
  <action>
Create `get-shit-done/bin/conversation-miner.js` as a CommonJS module (matching all other bin/ files). Include `'use strict';` at top.

The module exports four functions:

**1. `discoverProjectConversations(projectCwd, options = {})`**
- Parameters:
  - `projectCwd` (string): Absolute path to the project directory (e.g., `/Users/ollorin/get-shit-done`)
  - `options.maxAgeDays` (number, default 30): Skip files older than N days. The 30-day default matches the Phase 04-05 decision for session log age filtering.
  - `options.includeSubagents` (boolean, default false): Also scan `{sessionId}/subagents/*.jsonl` subdirectories. Default off per research recommendation (subagents are noisy).
- Logic:
  - Compute slug: `projectCwd.replace(/\//g, '-')` to get the Claude Code project directory slug
  - Build path: `path.join(os.homedir(), '.claude', 'projects', slug)`
  - If directory does not exist, return `{ files: [], projectSlugDir, error: 'Project slug directory not found' }`
  - Read directory entries. For each `.jsonl` file at top level:
    - `fs.statSync()` to get size and mtime
    - Skip if `mtime < cutoffMs` (where cutoff = `Date.now() - maxAgeDays * 86400000`, skip cutoff logic if maxAgeDays <= 0)
    - Add to files array: `{ path, sessionId: filename.replace('.jsonl', ''), size, mtime }`
  - If `includeSubagents`, also scan each non-jsonl directory entry for a `subagents/` subfolder containing `.jsonl` files. For subagent files, use sessionId format: `{parentDir}-sub-{filename.replace('.jsonl', '')}` and add `isSubagent: true`.
  - Sort files by mtime descending (newest first)
  - Return `{ files, projectSlugDir }`
- Dependencies: `require('os')`, `require('path')`, `require('fs')` -- all Node.js built-ins.

**2. `convertConversationEntries(jsonlEntries)`**
- Parameters: `jsonlEntries` (Array) -- raw parsed JSONL entry objects from a Claude Code conversation file
- Logic (follow the research pattern exactly):
  - Define `RELEVANT_TYPES = new Set(['user', 'assistant'])`
  - Iterate entries. Skip any entry where type is not in RELEVANT_TYPES.
  - For `type === 'user'`:
    - If `message.content` (or `content`) is an Array:
      - If ALL items have `type === 'tool_result'`, skip entirely (tool output, not human input)
      - Extract only items with `type === 'text'`, join their `.text` values with newlines, trim
      - If joined text < 20 chars, skip (minimal/empty message)
      - Strip XML tags and content: `text.replace(/<[^>]+>[\s\S]*?<\/[^>]+>/g, '').trim()`. If cleaned text < 10 chars, skip (pure GSD command injection)
      - Push `{ type: 'user_message', content: cleanText || texts, timestamp, original_type: 'user' }`
    - If content is a string:
      - Apply same 20-char minimum and XML stripping as above
      - Push `{ type: 'user_message', content, timestamp, original_type: 'user' }`
  - For `type === 'assistant'`:
    - Content is always `message.content` (under `message` key). Extract `message.content` array.
    - Filter to items with `type === 'text'`, join `.text` values, trim
    - If joined text < 30 chars, skip (brief orchestration notes)
    - Push `{ type: 'bot_response', content: texts, timestamp, original_type: 'assistant' }`
  - Get timestamp from `entry.timestamp || new Date().toISOString()`
  - Note: The user entry content may be at `entry.message.content` OR `entry.content` -- handle both. Check `entry.message?.content` first, fall back to `entry.content`.
  - Return the result array of converted entries.

**3. `shouldMineConversation(convertedEntries)`**
- Parameters: `convertedEntries` (Array) -- output of convertConversationEntries()
- Logic:
  - Count entries where `type === 'user_message'` (userMsgCount)
  - Count entries where `type === 'bot_response'` (botRespCount)
  - Sum all `(entry.content || '').length` across entries (totalText)
  - If `botRespCount < 2`: return `{ mine: false, reason: 'Only {botRespCount} assistant response(s) - too sparse' }`
  - If `totalText < 500`: return `{ mine: false, reason: 'Only {totalText} total chars - insufficient content' }`
  - Otherwise: return `{ mine: true, reason: '{userMsgCount} user msgs, {botRespCount} bot responses, {totalText} chars' }`
- CRITICAL: Do NOT call `shouldAnalyzeSession()` from session-quality-gates.js on conversation entries. That function checks for `question`/`answer` types which conversation entries never have. Use this dedicated function instead.

**4. `prepareConversationForMining(filePath, sessionId)`**
- Parameters:
  - `filePath` (string): Absolute path to JSONL file
  - `sessionId` (string): Session identifier (usually the filename without .jsonl)
- Logic:
  - Read the file: `fs.readFileSync(filePath, 'utf8')`, split by newlines, parse each line as JSON (skip malformed lines silently)
  - Call `convertConversationEntries(rawEntries)` to get converted entries
  - Call `shouldMineConversation(convertedEntries)` for quality check
  - If `!qualityResult.mine`: return `{ shouldMine: false, reason: qualityResult.reason }`
  - Compute content hash: `require('./session-quality-gates.js').getSessionContentHash(convertedEntries)`
  - Check re-analysis prevention using inline conversation log reading (do NOT call `isAlreadyAnalyzed()` from session-quality-gates.js -- that function reads `.planning/telegram-sessions/.analysis-log.jsonl`, the wrong log). Instead:
    - Resolve the conversation analysis log path: walk up from `__dirname` to find `.planning/` directory (same traversal pattern as `getAnalysisLogPath()` in session-quality-gates.js), then use `.planning/knowledge/.conversation-analysis-log.jsonl`
    - Read the log file with `fs.readFileSync(logPath, 'utf8')` (if file does not exist, treat as "not yet analyzed")
    - Split by newlines, parse each line as JSON, check if any entry has matching `session_id === sessionId` AND `content_hash === contentHash`
    - If match found: return `{ shouldMine: false, reason: 'Already analyzed (content hash matches)' }`
  - Call `require('./session-chunker.js').chunkSession(convertedEntries)` for chunking
  - For each chunk, call `require('./session-analyzer.js').analyzeSession(chunk)` to get extraction requests
  - Flatten all extraction requests into one array
  - Return `{ shouldMine: true, reason: qualityResult.reason, contentHash, chunkCount, extractionRequests }`
- Lazy-require Phase 11 modules (session-quality-gates for getSessionContentHash, session-chunker, session-analyzer) inside the function, not at module top level. This matches the lazy-loading pattern used throughout gsd-tools.js.

Module file header: Include the standard `#!/usr/bin/env node` shebang and JSDoc comment block explaining the module's purpose as a format adapter for Claude Code JSONL -> Phase 11 extraction pipeline.
  </action>
  <verify>
Run `node -e "const m = require('./get-shit-done/bin/conversation-miner.js'); console.log(Object.keys(m))"` from project root. Must output: `[ 'discoverProjectConversations', 'convertConversationEntries', 'shouldMineConversation', 'prepareConversationForMining' ]`.

Then verify discovery works: `node -e "const m = require('./get-shit-done/bin/conversation-miner.js'); const r = m.discoverProjectConversations(process.cwd()); console.log('files:', r.files.length, 'dir:', r.projectSlugDir)"` -- should find files in the expected slug directory.

Then verify conversion works: `node -e "const m = require('./get-shit-done/bin/conversation-miner.js'); const fs = require('fs'); const os = require('os'); const path = require('path'); const slug = process.cwd().replace(/\\//g, '-'); const dir = path.join(os.homedir(), '.claude', 'projects', slug); const files = fs.readdirSync(dir).filter(f => f.endsWith('.jsonl')); if (files[0]) { const entries = fs.readFileSync(path.join(dir, files[0]), 'utf8').split('\\n').filter(l=>l.trim()).map(l=>{try{return JSON.parse(l)}catch{return null}}).filter(Boolean); const converted = m.convertConversationEntries(entries); console.log('raw:', entries.length, 'converted:', converted.length, 'types:', [...new Set(converted.map(e=>e.type))]); }"` -- converted entries should have only user_message and bot_response types.
  </verify>
  <done>
conversation-miner.js exists at get-shit-done/bin/conversation-miner.js, exports all four functions, loads without errors, discovers JSONL files for the current project, converts raw entries to user_message/bot_response format filtering out 90%+ noise, and quality gate correctly rejects sparse conversations.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify format compatibility with Phase 11 pipeline</name>
  <files>get-shit-done/bin/conversation-miner.js</files>
  <action>
Run an end-to-end format compatibility check to confirm converted entries work through the full Phase 11 pipeline (formatEntriesForPrompt -> analyzeSession -> extraction requests).

Write and execute an inline Node.js script that:
1. Loads conversation-miner.js
2. Discovers the first JSONL file for the current project
3. Reads and parses the raw entries
4. Calls convertConversationEntries() to get converted entries
5. Passes converted entries to `require('./analysis-prompts.js').formatEntriesForPrompt()` -- verify the output is NOT "(no relevant entries...)" or "(empty session...)"
6. Passes converted entries to `require('./session-analyzer.js').analyzeSession()` -- verify it returns 3 extraction request objects (decision, reasoning_pattern, meta_knowledge)
7. Passes converted entries to `require('./session-chunker.js').chunkSession()` -- verify chunks are non-empty
8. Calls shouldMineConversation() on converted entries -- verify it returns mine:true for a non-trivial conversation

If any of these checks fail, fix the conversion logic in conversation-miner.js. Common issues:
- formatEntriesForPrompt returns "(no relevant entries...)" means converted entry types are wrong (must be exactly 'user_message' and 'bot_response')
- analyzeSession returns empty array means formatEntriesForPrompt returned empty text
- chunkSession returns empty chunks means no substantive entries were found

This is a validation task -- no new files created, only fixes to conversation-miner.js if needed.
  </action>
  <verify>
The inline script completes without errors and prints:
- formatEntriesForPrompt: non-empty session text (not placeholder text)
- analyzeSession: 3 extraction requests
- chunkSession: at least 1 chunk with entries
- shouldMineConversation: mine=true for a real conversation file
  </verify>
  <done>
Converted conversation entries flow through formatEntriesForPrompt(), analyzeSession(), and chunkSession() without errors. The format adapter produces entries that the Phase 11 pipeline accepts as valid input.
  </done>
</task>

</tasks>

<verification>
- conversation-miner.js loads without errors: `node -e "require('./get-shit-done/bin/conversation-miner.js')"`
- All four exports present: discoverProjectConversations, convertConversationEntries, shouldMineConversation, prepareConversationForMining
- Discovery finds JSONL files for current project at expected slug path
- Conversion produces entries with types recognized by formatEntriesForPrompt (user_message, bot_response)
- Quality gate rejects sparse conversations and passes substantive ones
- prepareConversationForMining returns extraction requests compatible with Task() subagent pattern
</verification>

<success_criteria>
conversation-miner.js is the working format adapter that bridges Claude Code JSONL format to the Phase 11 extraction pipeline. Discovery, conversion, quality gating, and extraction request preparation all function correctly with real project conversation data.
</success_criteria>

<output>
After completion, create `.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-01-SUMMARY.md`
</output>
