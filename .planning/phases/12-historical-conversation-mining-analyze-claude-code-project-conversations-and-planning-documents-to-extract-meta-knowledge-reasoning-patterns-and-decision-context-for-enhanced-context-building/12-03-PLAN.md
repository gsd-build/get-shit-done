---
phase: 12-historical-conversation-mining
plan: 03
type: execute
wave: 3
depends_on: ["12-02"]
files_modified:
  - get-shit-done/workflows/mine-conversations.md
autonomous: true

must_haves:
  truths:
    - "A GSD workflow exists that orchestrates end-to-end conversation mining"
    - "The workflow discovers conversations via mine-conversations CLI command"
    - "The workflow spawns Haiku Task() subagents for each extraction request (zero direct API calls)"
    - "The workflow stores results via store-conversation-result CLI command"
    - "The workflow reports summary statistics (sessions processed, insights stored/evolved/skipped, errors)"
    - "Already-analyzed conversations are skipped (no wasted Haiku calls)"
  artifacts:
    - path: "get-shit-done/workflows/mine-conversations.md"
      provides: "GSD workflow for conversation mining orchestration"
      contains: "mine-conversations"
      min_lines: 80
  key_links:
    - from: "get-shit-done/workflows/mine-conversations.md"
      to: "get-shit-done/bin/gsd-tools.js"
      via: "CLI invocation of mine-conversations and store-conversation-result commands"
      pattern: "gsd-tools.js mine-conversations"
    - from: "get-shit-done/workflows/mine-conversations.md"
      to: "Haiku Task() subagent"
      via: "Task(subagent_type=\"general-purpose\", model=\"haiku\") invocation pattern"
      pattern: "Task\\("
---

<objective>
Create the mine-conversations.md GSD workflow that orchestrates end-to-end conversation mining: discover conversations, spawn Haiku subagents for extraction, store results.

Purpose: This workflow is the user-facing entry point for conversation mining, following the exact same pattern as analyze-pending-sessions.md (Phase 11). It uses the mine-conversations CLI command to discover and prepare conversations, then spawns Haiku Task() subagents for each extraction request, and stores results via store-conversation-result.

Output: A complete workflow file at get-shit-done/workflows/mine-conversations.md.
</objective>

<execution_context>
@/Users/ollorin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/ollorin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-RESEARCH.md
@.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-01-SUMMARY.md
@.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-02-SUMMARY.md
@get-shit-done/workflows/analyze-pending-sessions.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create mine-conversations.md workflow</name>
  <files>get-shit-done/workflows/mine-conversations.md</files>
  <action>
Create `get-shit-done/workflows/mine-conversations.md` following the exact structure of `analyze-pending-sessions.md`.

The workflow should contain these sections:

**`<purpose>`**
Mine Claude Code project conversations to extract decisions, reasoning patterns, and meta-knowledge. This workflow discovers conversation JSONL files at ~/.claude/projects/{slug}/, converts them to session-like entries, spawns Haiku subagents for knowledge extraction, and stores results in the project's knowledge database.

Invocation:
- Manually: `/gsd:mine-conversations`
- With options: `/gsd:mine-conversations --max-age-days 7 --limit 5`
- Include subagent files: `/gsd:mine-conversations --include-subagents`

**`<constraints>`**
- ZERO direct API calls -- all Haiku work via Task(subagent_type="general-purpose", model="haiku")
- Sequential processing: one conversation at a time to avoid overwhelming the system
- Partial analysis is better than none: if one extraction type fails, continue with others
- If store-conversation-result fails for a conversation, log and continue with the next
- Default to current project only, last 30 days

**`<process>` with steps:**

**Step 1: `discover_conversations`**
```bash
MINE_JSON=$(node {gsd-tools-path}/gsd-tools.js mine-conversations --max-age-days 30 --limit 10)
```
Note: Use the actual absolute path `/Users/ollorin/.claude/get-shit-done/bin/gsd-tools.js` (matching the project's convention from analyze-pending-sessions.md which uses `/Users/ollorin/.claude/get-shit-done/bin/gsd-tools.js`).

Parse JSON output. Extract `status`, `sessionsReady`, `sessionsSkipped`, `sessions`, `skipped`.

If `status === 'error'`: report the error and exit.
If `sessionsReady === 0`: report "No conversations ready for mining" with count of skipped and their reasons. Exit.

Otherwise report: "Found {sessionsReady} conversation(s) ready for mining ({sessionsSkipped} skipped)."

If any skipped, briefly list reasons (grouped by reason if many).

**Step 2: `process_each_conversation` (for_each="session in sessions")**

For each session in the sessions array:

1. Log start: `Mining conversation {sessionId}...`

2. If `session.extractionRequests` is empty, skip with message.

3. For each extraction request in `session.extractionRequests`:
   Each has `{ type: string, prompt: string }`.

   Spawn Haiku subagent:
   ```
   Task(
     subagent_type="general-purpose",
     model="haiku",
     prompt="{request.prompt}"
   )
   ```

   Collect raw text output. If Task() throws or returns empty, log error and continue.

4. Assemble results array:
   ```json
   [
     {"type": "decision", "result": "{haikuOutput1}"},
     {"type": "reasoning_pattern", "result": "{haikuOutput2}"},
     {"type": "meta_knowledge", "result": "{haikuOutput3}"}
   ]
   ```

5. Store results:
   ```bash
   STORE_RESULT=$(node /Users/ollorin/.claude/get-shit-done/bin/gsd-tools.js \
     store-conversation-result "{sessionId}" '{resultsJson}' --content-hash "{session.contentHash}")
   ```

   Note: Pass the contentHash from the session object to --content-hash so the store command can mark this exact content version as analyzed.

6. Log outcome: `Conversation {sessionId}: {stored} stored, {evolved} evolved, {skipped} skipped, {errors.length} errors`

**Step 3: `summary`**

After all conversations processed:
```
Conversation Mining Complete
=============================
Conversations processed: {count}
Total insights stored: {totalStored}
Total insights evolved: {totalEvolved}
Total insights skipped (duplicates): {totalSkipped}
Errors encountered: {totalErrors}
```

If any conversations failed entirely, list them.

**`<success_criteria>` checklist:**
- All discovered conversations processed via Haiku Task() subagents
- Results stored via store-conversation-result
- Already-analyzed conversations skipped at discovery time (not re-processed)
- Summary report generated
- Failed conversations listed for manual retry
  </action>
  <verify>
1. Verify the workflow file exists and is well-formed markdown:
```bash
wc -l get-shit-done/workflows/mine-conversations.md
```
Should be 80+ lines.

2. Verify it references the correct gsd-tools.js path:
```bash
grep "gsd-tools.js mine-conversations" get-shit-done/workflows/mine-conversations.md
grep "gsd-tools.js store-conversation-result" get-shit-done/workflows/mine-conversations.md
```
Both should match.

3. Verify it uses Task() subagent pattern (not direct API calls):
```bash
grep -c "Task(" get-shit-done/workflows/mine-conversations.md
```
Should be >= 1.

4. Verify it does NOT contain @anthropic-ai/sdk or direct API patterns:
```bash
grep -c "anthropic" get-shit-done/workflows/mine-conversations.md
```
Should be 0.
  </verify>
  <done>
mine-conversations.md workflow exists, follows the analyze-pending-sessions.md pattern, uses mine-conversations and store-conversation-result CLI commands, spawns Haiku Task() subagents for extraction (zero direct API calls), processes conversations sequentially, and reports summary statistics.
  </done>
</task>

<task type="auto">
  <name>Task 2: End-to-end integration verification</name>
  <files>get-shit-done/bin/conversation-miner.js, get-shit-done/bin/gsd-tools.js</files>
  <action>
Run a full integration check of the entire Phase 12 pipeline to verify all components work together:

1. **Discovery check:**
```bash
node /Users/ollorin/get-shit-done/get-shit-done/bin/gsd-tools.js mine-conversations --limit 2
```
Verify: JSON output has `status: 'ready'`, `sessions` array is non-empty (at least 1 session ready for mining from the last 30 days).

2. **Extraction request check:**
From the mine-conversations output, verify each session in `sessions` has:
- `sessionId` (non-empty string)
- `extractionRequests` (array with 3+ items: decision, reasoning_pattern, meta_knowledge types)
- `chunkCount` (positive integer)

3. **Content hash uniqueness check:**
Run mine-conversations twice. Verify that the second run shows the same sessions as "ready" (since no results have been stored yet -- they should NOT be marked as already analyzed until store-conversation-result is called).

4. **Workflow file structure check:**
Verify mine-conversations.md has all required sections:
- `<purpose>` section
- `<constraints>` section
- `<process>` section with steps
- `<success_criteria>` section

5. **No Phase 11 infrastructure modifications check:**
Verify that none of the Phase 11 files were modified:
```bash
git diff --name-only get-shit-done/bin/session-analyzer.js get-shit-done/bin/session-quality-gates.js get-shit-done/bin/session-chunker.js get-shit-done/bin/analysis-prompts.js get-shit-done/bin/knowledge-writer.js get-shit-done/bin/historical-extract.js
```
Should show no changes (all Phase 11 infrastructure unchanged).

If any check fails, diagnose and fix the issue in the relevant file (conversation-miner.js or gsd-tools.js). Do NOT modify Phase 11 files.
  </action>
  <verify>
All five checks pass:
1. mine-conversations returns ready status with sessions
2. Each session has valid extraction requests
3. Re-analysis prevention only triggers after store-conversation-result
4. Workflow file has all required sections
5. No Phase 11 files modified
  </verify>
  <done>
End-to-end pipeline verified: discovery finds conversations, conversion produces valid entries, quality gates filter correctly, extraction requests are generated for Haiku, CLI commands work, workflow file is complete, and Phase 11 infrastructure is untouched.
  </done>
</task>

</tasks>

<verification>
- mine-conversations.md exists and follows analyze-pending-sessions.md pattern
- Workflow uses Task() subagent (zero direct API calls)
- Workflow references correct CLI commands (mine-conversations, store-conversation-result)
- Full pipeline works end-to-end: discovery -> conversion -> quality gate -> extraction requests -> CLI output
- Phase 11 infrastructure unchanged (no modifications to session-analyzer.js, session-quality-gates.js, session-chunker.js, analysis-prompts.js, knowledge-writer.js, historical-extract.js)
</verification>

<success_criteria>
Phase 12 is complete: conversation-miner.js converts Claude Code JSONL to session-like entries, gsd-tools.js provides mine-conversations and store-conversation-result commands, mine-conversations.md workflow orchestrates the full extraction loop with Haiku Task() subagents. The entire Phase 11 extraction pipeline is reused without modification.
</success_criteria>

<output>
After completion, create `.planning/phases/12-historical-conversation-mining-analyze-claude-code-project-conversations-and-planning-documents-to-extract-meta-knowledge-reasoning-patterns-and-decision-context-for-enhanced-context-building/12-03-SUMMARY.md`
</output>
